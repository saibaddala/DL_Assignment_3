{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11760006,"sourceType":"datasetVersion","datasetId":7382666},{"sourceId":11760408,"sourceType":"datasetVersion","datasetId":7382932}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport pandas as pd\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.nn.utils.rnn import pad_sequence\nimport copy\nfrom torch.utils.data import Dataset, DataLoader\nimport gc\nimport random\nimport wandb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-10T13:58:14.625296Z","iopub.execute_input":"2025-05-10T13:58:14.625635Z","iopub.status.idle":"2025-05-10T13:58:14.630258Z","shell.execute_reply.started":"2025-05-10T13:58:14.625617Z","shell.execute_reply":"2025-05-10T13:58:14.629466Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"wandb.login(key=\"3117f688d100f7889a8f97ba664299887fe48de1\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T13:58:14.631444Z","iopub.execute_input":"2025-05-10T13:58:14.631775Z","iopub.status.idle":"2025-05-10T13:58:14.694374Z","shell.execute_reply.started":"2025-05-10T13:58:14.631710Z","shell.execute_reply":"2025-05-10T13:58:14.693859Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"# Set the device for training to CUDA if available, otherwise CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\n# Define special tokens for start, end, and padding\nEND_TOKEN = '>'\nSTART_TOKEN = '<'\nPAD_TOKEN = '_'\n\n# Define the teacher forcing ratio, which determines the probability of using teacher forcing during training\nTEACHER_FORCING_RATIO = 0.5\n\n# Paths to the training, testing, and validation CSV files\ntrain_csv = \"../input/telugu/te_train.csv\"\ntest_csv = \"../input/telugu/te_test.csv\"\nval_csv = \"../input/telugu/te_val.csv\"\n\n# Load the training data from the CSV file into a DataFrame\ntrain_df = pd.read_csv(train_csv, header=None)\n\n# Separate the source and target sequences from the training DataFrame\ntrain_source, train_target = train_df[0].to_numpy(), train_df[1].to_numpy()\n\n# Load the testing and validation data from the respective CSV files into DataFrames\ntest_df = pd.read_csv(test_csv, header=None)\nval_df = pd.read_csv(val_csv, header=None)\n\n# Separate the source and target sequences from the validation DataFrame\nval_source, val_target = val_df[0].to_numpy(), val_df[1].to_numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T13:58:14.694938Z","iopub.execute_input":"2025-05-10T13:58:14.695094Z","iopub.status.idle":"2025-05-10T13:58:14.820674Z","shell.execute_reply.started":"2025-05-10T13:58:14.695081Z","shell.execute_reply":"2025-05-10T13:58:14.819591Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# Function to add padding to source sequences\ndef add_padding(source_data, MAX_LENGTH):\n    padded_source_strings = []\n    for i in range(len(source_data)):\n        # Add start and end tokens to source sequence\n        source_str = START_TOKEN + source_data[i] + END_TOKEN\n        # Truncate or pad source sequence\n        source_str = source_str[:MAX_LENGTH]\n        source_str += PAD_TOKEN * (MAX_LENGTH - len(source_str))\n\n        padded_source_strings.append(source_str)\n        \n    return padded_source_strings\n\n# Function to convert source strings to sequences of indexes\ndef generate_string_to_sequence(source_data, source_char_index_dict):\n    source_sequences = []\n    for i in range(len(source_data)):\n        # Convert characters to indexes using the provided dictionary\n        source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n    # Pad sequences to the same length\n    source_sequences = pad_sequence(source_sequences, batch_first=True, padding_value=2)\n    return source_sequences\n\n# Function to convert characters to their corresponding indexes\ndef get_chars(string, char_index_dict):\n    chars_indexes = []\n    for char in string:\n        # Map characters to their corresponding indexes using the provided dictionary\n        chars_indexes.append(char_index_dict[char])\n    return torch.tensor(chars_indexes, device=device)\n\n# Preprocess the data, including adding padding, generating sequences, and updating dictionaries\ndef preprocess_data(source_data, target_data):\n    data = {\n        \"source_chars\": [START_TOKEN, END_TOKEN, PAD_TOKEN],\n        \"target_chars\": [START_TOKEN, END_TOKEN, PAD_TOKEN],\n        \"source_char_index\": {START_TOKEN: 0, END_TOKEN: 1, PAD_TOKEN: 2},\n        \"source_index_char\": {0: START_TOKEN, 1: END_TOKEN, 2: PAD_TOKEN},\n        \"target_char_index\": {START_TOKEN: 0, END_TOKEN: 1, PAD_TOKEN: 2},\n        \"target_index_char\": {0: START_TOKEN, 1: END_TOKEN, 2: PAD_TOKEN},\n        \"source_len\": 3,\n        \"target_len\": 3,\n        \"source_data\": source_data,\n        \"target_data\": target_data,\n        \"source_data_seq\": [],\n        \"target_data_seq\": []\n    }\n    \n    # Calculate the maximum length of input and output sequences\n    data[\"INPUT_MAX_LENGTH\"] = max(len(string) for string in source_data) + 2\n    data[\"OUTPUT_MAX_LENGTH\"] = max(len(string) for string in target_data) + 2\n\n    # Pad the source and target sequences and update character dictionaries\n    padded_source_strings = add_padding(source_data, data[\"INPUT_MAX_LENGTH\"])\n    padded_target_strings = add_padding(target_data, data[\"OUTPUT_MAX_LENGTH\"])\n    \n    for i in range(len(padded_source_strings)):\n        for c in padded_source_strings[i]:\n            if data[\"source_char_index\"].get(c) is None:\n                data[\"source_chars\"].append(c)\n                idx = len(data[\"source_chars\"]) - 1\n                data[\"source_char_index\"][c] = idx\n                data[\"source_index_char\"][idx] = c\n        for c in padded_target_strings[i]:\n            if data[\"target_char_index\"].get(c) is None:\n                data[\"target_chars\"].append(c)\n                idx = len(data[\"target_chars\"]) - 1\n                data[\"target_char_index\"][c] = idx\n                data[\"target_index_char\"][idx] = c\n\n    # Generate sequences of indexes for source and target data\n    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings, data['source_char_index'])\n    data['target_data_seq'] = generate_string_to_sequence(padded_target_strings, data['target_char_index'])\n    \n\n    # Update lengths of source and target character lists\n    data[\"source_len\"] = len(data[\"source_chars\"])\n    data[\"target_len\"] = len(data[\"target_chars\"])\n    \n    return data\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T13:58:14.822996Z","iopub.execute_input":"2025-05-10T13:58:14.823265Z","iopub.status.idle":"2025-05-10T13:58:14.838866Z","shell.execute_reply.started":"2025-05-10T13:58:14.823239Z","shell.execute_reply":"2025-05-10T13:58:14.837949Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Function to get the appropriate cell type based on the input string\ndef get_cell_type(cell_type):\n    if cell_type == \"RNN\":\n        return nn.RNN\n    elif cell_type == \"LSTM\":\n        return nn.LSTM\n    elif cell_type == \"GRU\":\n        return nn.GRU\n    else:\n        print(\"Specify correct cell type\")\n\n# Encoder module for the seq2seq model\nclass Encoder(nn.Module):\n    def __init__(self, h_params, data, device):\n        super(Encoder, self).__init__()\n        # Embedding layer for input data\n        self.embedding = nn.Embedding(data[\"source_len\"], h_params[\"char_embd_dim\"])\n        # Dropout layer for regularization\n        self.dropout = nn.Dropout(h_params[\"dropout\"])\n        # Cell type chosen for encoding\n        self.cell = get_cell_type(h_params[\"cell_type\"])(h_params[\"char_embd_dim\"], h_params[\"hidden_layer_neurons\"],\n                                                         num_layers=h_params[\"number_of_layers\"], dropout=h_params[\"dropout\"], batch_first=True)\n        self.device = device\n        self.h_params = h_params\n\n    def forward(self, current_input, prev_state):\n        # Embedding input\n        embd_input = self.embedding(current_input)\n        embd_input = self.dropout(embd_input)\n        # Encoding step\n        output, prev_state = self.cell(embd_input, prev_state)\n        return output, prev_state\n\n    # Initialize initial state of the encoder\n    def getInitialState(self):\n        return torch.zeros(self.h_params[\"number_of_layers\"], self.h_params[\"batch_size\"], self.h_params[\"hidden_layer_neurons\"], device=self.device)\n\n# Decoder module for the seq2seq model\nclass Decoder(nn.Module):\n    def __init__(self, h_params, data, device):\n        super(Decoder, self).__init__()\n        # Embedding layer for target data\n        self.embedding = nn.Embedding(data[\"target_len\"], h_params[\"char_embd_dim\"])\n        # Dropout layer for regularization\n        self.dropout = nn.Dropout(h_params[\"dropout\"])\n        # Cell type chosen for decoding\n        self.cell = get_cell_type(h_params[\"cell_type\"])(h_params[\"char_embd_dim\"], h_params[\"hidden_layer_neurons\"],\n                                                         num_layers=h_params[\"number_of_layers\"], dropout=h_params[\"dropout\"], batch_first=True)\n        # Fully connected layer for output\n        self.fc = nn.Linear(h_params[\"hidden_layer_neurons\"], data[\"target_len\"])\n        # Softmax layer for probability distribution\n        self.softmax = nn.LogSoftmax(dim=2)\n        self.h_params = h_params\n\n    def forward(self, current_input, prev_state):\n        # Embedding input\n        embd_input = self.embedding(current_input)\n        curr_embd = F.relu(embd_input)\n        curr_embd = self.dropout(curr_embd)\n        # Decoding step\n        output, prev_state = self.cell(curr_embd, prev_state)\n        # Apply softmax to output\n        output = self.softmax(self.fc(output))\n        return output, prev_state\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T13:58:14.839810Z","iopub.execute_input":"2025-05-10T13:58:14.840046Z","iopub.status.idle":"2025-05-10T13:58:14.865006Z","shell.execute_reply.started":"2025-05-10T13:58:14.840030Z","shell.execute_reply":"2025-05-10T13:58:14.864181Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# Custom dataset class for handling source and target sequences\nclass MyDataset(Dataset):\n    def __init__(self, data):\n        # Initialize with source and target sequences\n        self.source_data_seq = data[0]\n        self.target_data_seq = data[1]\n    \n    def __len__(self):\n        # Return the length of the dataset\n        return len(self.source_data_seq)\n    \n    def __getitem__(self, idx):\n        # Get source and target data at the specified index\n        source_data = self.source_data_seq[idx]\n        target_data = self.target_data_seq[idx]\n        return source_data, target_data\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T13:58:14.865833Z","iopub.execute_input":"2025-05-10T13:58:14.866081Z","iopub.status.idle":"2025-05-10T13:58:14.888718Z","shell.execute_reply.started":"2025-05-10T13:58:14.866061Z","shell.execute_reply":"2025-05-10T13:58:14.888107Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Function for inference on the trained model\ndef inference(encoder, decoder, source_sequence, target_tensor, data, device, h_params, loss_fn, batch_num):\n    # Set encoder and decoder to evaluation mode\n    encoder.eval()\n    decoder.eval()\n    \n    # Initialize loss and correct predictions\n    loss = 0\n    correct = 0\n    \n    # Turn off gradient calculation\n    with torch.no_grad():\n        # Initialize encoder hidden state\n        encoder_hidden = encoder.getInitialState()\n        # For LSTM, initialize cell state as well\n        if h_params[\"cell_type\"] == \"LSTM\":\n            encoder_hidden = (encoder_hidden, encoder.getInitialState())\n        # Perform encoding\n        encoder_outputs, encoder_hidden = encoder(source_sequence, encoder_hidden)\n\n        # Initialize decoder input with start token\n        decoder_input_tensor = torch.full((h_params[\"batch_size\"], 1), data['target_char_index'][START_TOKEN], device=device)\n        decoder_actual_output = []\n        \n        # Initialize decoder hidden state with encoder hidden state\n        decoder_hidden = encoder_hidden\n        \n        # Iterate over each output time step\n        for di in range(data[\"OUTPUT_MAX_LENGTH\"]):\n            curr_target_chars = target_tensor[:, di]\n            # Perform decoding for one time step\n            decoder_output, decoder_hidden = decoder(decoder_input_tensor, decoder_hidden)\n            topv, topi = decoder_output.topk(1)\n            decoder_input_tensor = topi.squeeze().detach()\n            decoder_actual_output.append(decoder_input_tensor)\n            decoder_input_tensor = decoder_input_tensor.view(h_params[\"batch_size\"], 1)\n                        \n            decoder_output = decoder_output[:, -1, :]\n            # Compute loss for the current time step\n            loss += (loss_fn(decoder_output, curr_target_chars))\n\n        # Concatenate decoder outputs\n        decoder_actual_output = torch.cat(decoder_actual_output, dim=0).view(data[\"OUTPUT_MAX_LENGTH\"], h_params[\"batch_size\"]).transpose(0, 1)\n\n        # Compute number of correct predictions\n        correct = (decoder_actual_output == target_tensor).all(dim=1).sum().item()\n\n        return correct, loss.item() / data[\"OUTPUT_MAX_LENGTH\"]\n\n# Function to evaluate the model on validation or test data\ndef evaluate(encoder, decoder, data, dataloader, device, h_params, loss_fn):\n    correct_predictions = 0\n    total_loss = 0\n    total_predictions = len(dataloader.dataset)\n    number_of_batches = len(dataloader)\n    \n    # Iterate over each batch\n    for batch_num, (source_sequence, target_sequence) in enumerate(dataloader):\n        input_tensor = source_sequence\n        target_tensor = target_sequence\n        \n        # Perform inference on the batch\n        correct, loss = inference(encoder, decoder, input_tensor, target_tensor, data, device, h_params, loss_fn, batch_num)\n        \n        correct_predictions += correct\n        total_loss += loss\n    \n    # Compute accuracy and average loss\n    accuracy = correct_predictions / total_predictions\n    total_loss /= number_of_batches\n    \n    return accuracy, total_loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T13:58:14.889441Z","iopub.execute_input":"2025-05-10T13:58:14.889663Z","iopub.status.idle":"2025-05-10T13:58:14.909280Z","shell.execute_reply.started":"2025-05-10T13:58:14.889645Z","shell.execute_reply":"2025-05-10T13:58:14.908550Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# Function to create strings from indexes\ndef make_strings(data, source, target, output):\n    source_string = \"\"\n    target_string = \"\"\n    output_string = \"\"\n    # Convert source indexes to characters\n    for i in source:\n        source_string += (data['source_index_char'][i.item()])\n    # Convert target indexes to characters\n    for i in target:\n        target_string += (data['target_index_char'][i.item()])\n    # Convert output indexes to characters\n    for i in output:\n        output_string += (data['target_index_char'][i.item()])\n    return source_string, target_string, output_string\n\n# Training loop\ndef train_loop(encoder, decoder, h_params, data, data_loader, val_dataloader, device):\n    # Choose optimizer based on the specified type\n    if h_params[\"optimizer\"] == \"adam\":\n        encoder_optimizer = optim.Adam(encoder.parameters(), lr=h_params[\"learning_rate\"])\n        decoder_optimizer = optim.Adam(decoder.parameters(), lr=h_params[\"learning_rate\"])\n    elif h_params[\"optimizer\"] == \"nadam\":\n        encoder_optimizer = optim.NAdam(encoder.parameters(), lr=h_params[\"learning_rate\"])\n        decoder_optimizer = optim.NAdam(decoder.parameters(), lr=h_params[\"learning_rate\"])\n    \n    total_predictions = len(data_loader.dataset)\n    total_batches = len(data_loader)\n    loss_fn = nn.NLLLoss()\n    \n    # Loop through each epoch\n    for ep in range(h_params[\"epochs\"]):\n        encoder.train()\n        decoder.train()\n        total_loss = 0\n        total_correct = 0\n        \n        # Loop through each batch\n        for batch_num, (source_batch, target_batch) in enumerate(data_loader):\n            # Get initial state of the encoder\n            encoder_initial_state = encoder.getInitialState()\n            if h_params[\"cell_type\"] == \"LSTM\":\n                encoder_initial_state = (encoder_initial_state, encoder.getInitialState())\n            \n            encoder_current_state = encoder_initial_state\n            encoder_output, encoder_current_state = encoder(source_batch, encoder_current_state)\n            loss = 0\n            correct = 0\n            decoder_curr_state = encoder_current_state\n            output_seq_len = data[\"OUTPUT_MAX_LENGTH\"]\n            decoder_actual_output = []\n            \n            # Determine whether to use teacher forcing\n            use_teacher_forcing = True if random.random() < TEACHER_FORCING_RATIO else False\n\n            # Loop through each output time step\n            for i in range(data[\"OUTPUT_MAX_LENGTH\"]):\n                if(i == 0):\n                    decoder_input_tensor = target_batch[:, i].view(h_params[\"batch_size\"], 1)\n                curr_target_chars = target_batch[:, i]\n                decoder_output, decoder_curr_state = decoder(decoder_input_tensor, decoder_curr_state)\n                topv, topi = decoder_output.topk(1)\n                decoder_input_tensor = topi.squeeze().detach()\n                decoder_actual_output.append(decoder_input_tensor)\n                if(i < output_seq_len - 1):\n                    if use_teacher_forcing:\n                        decoder_input_tensor = target_batch[:, i + 1].view(h_params[\"batch_size\"], 1)\n                    else:\n                        decoder_input_tensor = decoder_input_tensor.view(h_params[\"batch_size\"], 1)\n                decoder_output = decoder_output[:, -1, :]\n                loss += (loss_fn(decoder_output, curr_target_chars))\n\n            decoder_actual_output = torch.cat(decoder_actual_output, dim=0).view(output_seq_len, h_params[\"batch_size\"]).transpose(0, 1)\n            \n            correct = (decoder_actual_output == target_batch).all(dim=1).sum().item()\n            total_correct += correct\n            total_loss += loss.item() / output_seq_len\n            \n            # Backpropagation and optimization step\n            encoder_optimizer.zero_grad()\n            decoder_optimizer.zero_grad()\n            loss.backward()\n            encoder_optimizer.step()\n            decoder_optimizer.step()\n        \n        train_acc = total_correct / total_predictions\n        train_loss = total_loss / total_batches\n        \n        # Evaluate on validation set\n        val_acc, val_loss = evaluate(encoder, decoder, data, val_dataloader, device, h_params, loss_fn)\n        \n        # Log metrics\n        print(\"ep: \", ep, \" train acc:\", train_acc, \" train loss:\", train_loss, \" val acc:\", val_acc, \" val loss:\", val_loss)\n        wandb.log({\"train_accuracy\": train_acc, \"train_loss\": train_loss, \"val_accuracy\": val_acc, \"val_loss\": val_loss, \"epoch\": ep})\n\n    return encoder, decoder, loss_fn\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T13:58:14.910068Z","iopub.execute_input":"2025-05-10T13:58:14.910420Z","iopub.status.idle":"2025-05-10T13:58:14.931090Z","shell.execute_reply.started":"2025-05-10T13:58:14.910398Z","shell.execute_reply":"2025-05-10T13:58:14.930343Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# Training function\ndef train(h_params, data, device, train_dataloader, val_dataloader):\n    # Initialize encoder and decoder\n    encoder = Encoder(h_params, data, device).to(device)\n    decoder = Decoder(h_params, data, device).to(device)\n    \n    # Perform training loop\n    encoder, decoder, loss_fn = train_loop(encoder, decoder, h_params, data, train_dataloader, val_dataloader, device)\n    return encoder, decoder, loss_fn\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T13:58:14.931809Z","iopub.execute_input":"2025-05-10T13:58:14.932105Z","iopub.status.idle":"2025-05-10T13:58:14.949723Z","shell.execute_reply.started":"2025-05-10T13:58:14.932084Z","shell.execute_reply":"2025-05-10T13:58:14.948984Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Function to prepare dataloaders for training and validation\ndef prepare_dataloaders(train_source, train_target, val_source, val_target, h_params):\n    # Preprocess training data\n    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n    training_data = [data[\"source_data_seq\"], data['target_data_seq']]\n    # Create training dataset and dataloader\n    train_dataset = MyDataset(training_data)\n    train_dataloader = DataLoader(train_dataset, batch_size=h_params[\"batch_size\"], shuffle=True)\n\n    # Preprocess validation data\n    val_padded_source_strings = add_padding(val_source, data[\"INPUT_MAX_LENGTH\"])\n    val_padded_target_strings = add_padding(val_target, data[\"OUTPUT_MAX_LENGTH\"])\n    val_source_sequences = generate_string_to_sequence(val_padded_source_strings, data['source_char_index'])\n    val_target_sequences = generate_string_to_sequence(val_padded_target_strings, data['target_char_index'])\n    validation_data = [val_source_sequences, val_target_sequences]\n    # Create validation dataset and dataloader\n    val_dataset = MyDataset(validation_data)\n    val_dataloader = DataLoader(val_dataset, batch_size=h_params[\"batch_size\"], shuffle=True)\n    return train_dataloader, val_dataloader, data\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T13:58:14.951612Z","iopub.execute_input":"2025-05-10T13:58:14.951869Z","iopub.status.idle":"2025-05-10T13:58:14.965720Z","shell.execute_reply.started":"2025-05-10T13:58:14.951854Z","shell.execute_reply":"2025-05-10T13:58:14.965013Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"#Run this cell to run a sweep with appropriate parameters\nsweep_params = {\n    'method' : 'bayes',\n    'name'   : 'DL assignment 3 sweep',\n    'metric' : {\n        'goal' : 'maximize',\n        'name' : 'val_accuracy',\n    },\n    'parameters' : {\n        'epochs':{'values' : [15, 20]},\n        'learning_rate':{'values' : [0.001, 0.0001]},\n        'batch_size':{'values':[32,64, 128]},\n        'char_embd_dim':{'values' : [64, 128, 256] } ,\n        'number_of_layers':{'values' : [1,2,3,4]},\n        'optimizer':{'values':['nadam','adam']},\n        'cell_type':{'values' : [\"RNN\",\"LSTM\", \"GRU\"]},\n        'hidden_layer_neurons':{'values': [ 128, 256, 512]},\n        'dropout':{'values': [0,0.2, 0.3]}\n    }\n}\n\nsweep_id = wandb.sweep(sweep=sweep_params, project=\"DL Assignment 3\")\ndef main():\n    wandb.init(project=\"DL Assignment 3\" )\n    config = wandb.config\n    with wandb.init(project=\"DL Assignment 3\", name=f\"{config['cell_type']}_{config['optimizer']}_ep_{config['epochs']}_lr_{config['learning_rate']}_embd_{config['char_embd_dim']}_hid_lyr_neur_{config['hidden_layer_neurons']}_bs_{config['batch_size']}_enc_layers_{config['number_of_layers']}_dec_layers_{config['number_of_layers']}_dropout_{config['dropout']}\", config=config):\n        train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n        train(config, data, device, train_dataloader, val_dataloader)\nwandb.agent(sweep_id, function=main)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T14:00:01.100838Z","iopub.execute_input":"2025-05-10T14:00:01.101312Z","iopub.status.idle":"2025-05-10T14:00:30.654047Z","shell.execute_reply.started":"2025-05-10T14:00:01.101284Z","shell.execute_reply":"2025-05-10T14:00:30.653393Z"}},"outputs":[{"name":"stdout","text":"Create sweep with ID: 45b7s9os\nSweep URL: https://wandb.ai/cs23m059-iit-madras/DL%20Assignment%203/sweeps/45b7s9os\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: idj11zzw with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'DL Assignment 3' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250510_140007-idj11zzw</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cs23m059-iit-madras/DL%20Assignment%203/runs/idj11zzw' target=\"_blank\">trim-sweep-1</a></strong> to <a href='https://wandb.ai/cs23m059-iit-madras/DL%20Assignment%203' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23m059-iit-madras/DL%20Assignment%203/sweeps/45b7s9os' target=\"_blank\">https://wandb.ai/cs23m059-iit-madras/DL%20Assignment%203/sweeps/45b7s9os</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cs23m059-iit-madras/DL%20Assignment%203' target=\"_blank\">https://wandb.ai/cs23m059-iit-madras/DL%20Assignment%203</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/cs23m059-iit-madras/DL%20Assignment%203/sweeps/45b7s9os' target=\"_blank\">https://wandb.ai/cs23m059-iit-madras/DL%20Assignment%203/sweeps/45b7s9os</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cs23m059-iit-madras/DL%20Assignment%203/runs/idj11zzw' target=\"_blank\">https://wandb.ai/cs23m059-iit-madras/DL%20Assignment%203/runs/idj11zzw</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'DL Assignment 3' when running a sweep."},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/3175770146.py\", line 27, in main\n    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/3637728764.py\", line 4, in prepare_dataloaders\n    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/3500845976.py\", line 52, in preprocess_data\n    data[\"OUTPUT_MAX_LENGTH\"] = max(len(string) for string in target_data) + 2\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/3500845976.py\", line 52, in <genexpr>\n    data[\"OUTPUT_MAX_LENGTH\"] = max(len(string) for string in target_data) + 2\n                                    ^^^^^^^^^^^\nTypeError: object of type 'float' has no len()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">trim-sweep-1</strong> at: <a href='https://wandb.ai/cs23m059-iit-madras/DL%20Assignment%203/runs/idj11zzw' target=\"_blank\">https://wandb.ai/cs23m059-iit-madras/DL%20Assignment%203/runs/idj11zzw</a><br> View project at: <a href='https://wandb.ai/cs23m059-iit-madras/DL%20Assignment%203' target=\"_blank\">https://wandb.ai/cs23m059-iit-madras/DL%20Assignment%203</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250510_140007-idj11zzw/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run idj11zzw errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/3175770146.py\", line 27, in main\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/3637728764.py\", line 4, in prepare_dataloaders\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/3500845976.py\", line 52, in preprocess_data\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data[\"OUTPUT_MAX_LENGTH\"] = max(len(string) for string in target_data) + 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/3500845976.py\", line 52, in <genexpr>\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data[\"OUTPUT_MAX_LENGTH\"] = max(len(string) for string in target_data) + 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                                     ^^^^^^^^^^^\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m TypeError: object of type 'float' has no len()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: zf04oc6s with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'DL Assignment 3' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250510_140022-zf04oc6s</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cs23m059-iit-madras/DL%20Assignment%203/runs/zf04oc6s' target=\"_blank\">polar-sweep-2</a></strong> to <a href='https://wandb.ai/cs23m059-iit-madras/DL%20Assignment%203' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23m059-iit-madras/DL%20Assignment%203/sweeps/45b7s9os' target=\"_blank\">https://wandb.ai/cs23m059-iit-madras/DL%20Assignment%203/sweeps/45b7s9os</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cs23m059-iit-madras/DL%20Assignment%203' target=\"_blank\">https://wandb.ai/cs23m059-iit-madras/DL%20Assignment%203</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/cs23m059-iit-madras/DL%20Assignment%203/sweeps/45b7s9os' target=\"_blank\">https://wandb.ai/cs23m059-iit-madras/DL%20Assignment%203/sweeps/45b7s9os</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cs23m059-iit-madras/DL%20Assignment%203/runs/zf04oc6s' target=\"_blank\">https://wandb.ai/cs23m059-iit-madras/DL%20Assignment%203/runs/zf04oc6s</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'DL Assignment 3' when running a sweep."},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/3175770146.py\", line 27, in main\n    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/3637728764.py\", line 4, in prepare_dataloaders\n    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/3500845976.py\", line 52, in preprocess_data\n    data[\"OUTPUT_MAX_LENGTH\"] = max(len(string) for string in target_data) + 2\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/3500845976.py\", line 52, in <genexpr>\n    data[\"OUTPUT_MAX_LENGTH\"] = max(len(string) for string in target_data) + 2\n                                    ^^^^^^^^^^^\nTypeError: object of type 'float' has no len()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">polar-sweep-2</strong> at: <a href='https://wandb.ai/cs23m059-iit-madras/DL%20Assignment%203/runs/zf04oc6s' target=\"_blank\">https://wandb.ai/cs23m059-iit-madras/DL%20Assignment%203/runs/zf04oc6s</a><br> View project at: <a href='https://wandb.ai/cs23m059-iit-madras/DL%20Assignment%203' target=\"_blank\">https://wandb.ai/cs23m059-iit-madras/DL%20Assignment%203</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250510_140022-zf04oc6s/logs</code>"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"sweep_params = {\n    'method': 'bayes',\n    'name': 'DA6401_DL_Assignment_3',\n    'metric': {\n        'goal': 'maximize',\n        'name': 'val_accuracy',\n    },\n    'parameters': {\n        'epochs': {'values': [15, 20]},\n        'learning_rate': {'values': [0.001, 0.0001]},\n        'batch_size': {'values': [32, 64, 128]},\n        'char_embd_dim': {'values': [64, 128, 256]},\n        'number_of_layers': {'values': [1, 2, 3, 4]},\n        'optimizer': {'values': ['nadam', 'adam']},\n        'cell_type': {'values': ['RNN', 'LSTM', 'GRU']},\n        'hidden_layer_neurons': {'values': [128, 256, 512]},\n        'dropout': {'values': [0, 0.2, 0.3]}\n    }\n}\n\nsweep_id = wandb.sweep(sweep=sweep_params, project=\"DA6401_DL_Assignment_3\")\n\ndef main():\n    with wandb.init(project=\"DA6401_DL_Assignment_3\") as run:\n        config = wandb.config\n        run.name = f\"{config['cell_type']}_{config['optimizer']}_ep_{config['epochs']}_lr_{config['learning_rate']}_embd_{config['char_embd_dim']}_hid_lyr_neur_{config['hidden_layer_neurons']}_bs_{config['batch_size']}_enc_layers_{config['number_of_layers']}_dec_layers_{config['number_of_layers']}_dropout_{config['dropout']}\"\n\n        train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n        train(config, data, device, train_dataloader, val_dataloader)\n        \nwandb.agent(sweep_id, function=main)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T13:58:14.966353Z","iopub.execute_input":"2025-05-10T13:58:14.966554Z","iopub.status.idle":"2025-05-10T13:58:26.963440Z","shell.execute_reply.started":"2025-05-10T13:58:14.966540Z","shell.execute_reply":"2025-05-10T13:58:26.962720Z"}},"outputs":[{"name":"stdout","text":"Create sweep with ID: uaghmrx7\nSweep URL: https://wandb.ai/cs23m059-iit-madras/DA6401_DL_Assignment_3/sweeps/uaghmrx7\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: y1o0n24k with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n\u001b[34m\u001b[1mwandb\u001b[0m: \tchar_embd_dim: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_neurons: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'DA6401_DL_Assignment_3' when running a sweep."},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250510_135823-y1o0n24k</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cs23m059-iit-madras/DA6401_DL_Assignment_3/runs/y1o0n24k' target=\"_blank\">dutiful-sweep-1</a></strong> to <a href='https://wandb.ai/cs23m059-iit-madras/DA6401_DL_Assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23m059-iit-madras/DA6401_DL_Assignment_3/sweeps/uaghmrx7' target=\"_blank\">https://wandb.ai/cs23m059-iit-madras/DA6401_DL_Assignment_3/sweeps/uaghmrx7</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cs23m059-iit-madras/DA6401_DL_Assignment_3' target=\"_blank\">https://wandb.ai/cs23m059-iit-madras/DA6401_DL_Assignment_3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/cs23m059-iit-madras/DA6401_DL_Assignment_3/sweeps/uaghmrx7' target=\"_blank\">https://wandb.ai/cs23m059-iit-madras/DA6401_DL_Assignment_3/sweeps/uaghmrx7</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cs23m059-iit-madras/DA6401_DL_Assignment_3/runs/y1o0n24k' target=\"_blank\">https://wandb.ai/cs23m059-iit-madras/DA6401_DL_Assignment_3/runs/y1o0n24k</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/2579476246.py\", line 28, in main\n    train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/3637728764.py\", line 4, in prepare_dataloaders\n    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/3500845976.py\", line 52, in preprocess_data\n    data[\"OUTPUT_MAX_LENGTH\"] = max(len(string) for string in target_data) + 2\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_31/3500845976.py\", line 52, in <genexpr>\n    data[\"OUTPUT_MAX_LENGTH\"] = max(len(string) for string in target_data) + 2\n                                    ^^^^^^^^^^^\nTypeError: object of type 'float' has no len()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">RNN_nadam_ep_15_lr_0.0001_embd_64_hid_lyr_neur_128_bs_64_enc_layers_1_dec_layers_1_dropout_0.2</strong> at: <a href='https://wandb.ai/cs23m059-iit-madras/DA6401_DL_Assignment_3/runs/y1o0n24k' target=\"_blank\">https://wandb.ai/cs23m059-iit-madras/DA6401_DL_Assignment_3/runs/y1o0n24k</a><br> View project at: <a href='https://wandb.ai/cs23m059-iit-madras/DA6401_DL_Assignment_3' target=\"_blank\">https://wandb.ai/cs23m059-iit-madras/DA6401_DL_Assignment_3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250510_135823-y1o0n24k/logs</code>"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}