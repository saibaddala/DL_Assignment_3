{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11760927,"sourceType":"datasetVersion","datasetId":7383302}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport pandas as pd\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.nn.utils.rnn import pad_sequence\nimport copy\nfrom torch.utils.data import Dataset, DataLoader\nimport gc\nimport random\nimport wandb\n\nwandb.login(key=\"3117f688d100f7889a8f97ba664299887fe48de1\")\n\n# Set the device for training to CUDA if available, otherwise CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\n# Define special tokens for start, end, and padding\nEND_TOKEN = '>'\nSTART_TOKEN = '<'\nPAD_TOKEN = '_'\n\n# Define the teacher forcing ratio, which determines the probability of using teacher forcing during training\nTEACHER_FORCING_RATIO = 0.5\n\n# Paths to the training, testing, and validation CSV files\ntrain_csv = \"/kaggle/input/telugu/te_train.csv\"\ntest_csv = \"/kaggle/input/telugu/te_test.csv\"\nval_csv = \"/kaggle/input/telugu/te_val.csv\"\n\n# Load the training data from the CSV file into a DataFrame\ntrain_df = pd.read_csv(train_csv, header=None)\n\n# Separate the source and target sequences from the training DataFrame\ntrain_source, train_target = train_df[0].to_numpy(), train_df[1].to_numpy()\n\n# Load the testing and validation data from the respective CSV files into DataFrames\ntest_df = pd.read_csv(test_csv, header=None)\nval_df = pd.read_csv(val_csv, header=None)\n\n# Separate the source and target sequences from the validation DataFrame\nval_source, val_target = val_df[0].to_numpy(), val_df[1].to_numpy()\n\n# Function to add padding to source sequences\ndef add_padding(source_data, MAX_LENGTH):\n    padded_source_strings = []\n    for i in range(len(source_data)):\n        # Add start and end tokens to source sequence\n        source_str = START_TOKEN + source_data[i] + END_TOKEN\n        # Truncate or pad source sequence\n        source_str = source_str[:MAX_LENGTH]\n        source_str += PAD_TOKEN * (MAX_LENGTH - len(source_str))\n\n        padded_source_strings.append(source_str)\n        \n    return padded_source_strings\n\n# Function to convert source strings to sequences of indexes\ndef generate_string_to_sequence(source_data, source_char_index_dict):\n    source_sequences = []\n    for i in range(len(source_data)):\n        # Convert characters to indexes using the provided dictionary\n        source_sequences.append(get_chars(source_data[i], source_char_index_dict))\n    # Pad sequences to the same length\n    source_sequences = pad_sequence(source_sequences, batch_first=True, padding_value=2)\n    return source_sequences\n\n# Function to convert characters to their corresponding indexes\ndef get_chars(string, char_index_dict):\n    chars_indexes = []\n    for char in string:\n        # Map characters to their corresponding indexes using the provided dictionary\n        chars_indexes.append(char_index_dict[char])\n    return torch.tensor(chars_indexes, device=device)\n\n# Preprocess the data, including adding padding, generating sequences, and updating dictionaries\ndef preprocess_data(source_data, target_data):\n    data = {\n        \"source_chars\": [START_TOKEN, END_TOKEN, PAD_TOKEN],\n        \"target_chars\": [START_TOKEN, END_TOKEN, PAD_TOKEN],\n        \"source_char_index\": {START_TOKEN: 0, END_TOKEN: 1, PAD_TOKEN: 2},\n        \"source_index_char\": {0: START_TOKEN, 1: END_TOKEN, 2: PAD_TOKEN},\n        \"target_char_index\": {START_TOKEN: 0, END_TOKEN: 1, PAD_TOKEN: 2},\n        \"target_index_char\": {0: START_TOKEN, 1: END_TOKEN, 2: PAD_TOKEN},\n        \"source_len\": 3,\n        \"target_len\": 3,\n        \"source_data\": source_data,\n        \"target_data\": target_data,\n        \"source_data_seq\": [],\n        \"target_data_seq\": []\n    }\n    \n    # Calculate the maximum length of input and output sequences\n    data[\"INPUT_MAX_LENGTH\"] = max(len(string) for string in source_data) + 20\n    data[\"OUTPUT_MAX_LENGTH\"] = max(len(string) for string in target_data) + 20\n\n    # Pad the source and target sequences and update character dictionaries\n    padded_source_strings = add_padding(source_data, data[\"INPUT_MAX_LENGTH\"])\n    padded_target_strings = add_padding(target_data, data[\"OUTPUT_MAX_LENGTH\"])\n    \n    for i in range(len(padded_source_strings)):\n        for c in padded_source_strings[i]:\n            if data[\"source_char_index\"].get(c) is None:\n                data[\"source_chars\"].append(c)\n                idx = len(data[\"source_chars\"]) - 1\n                data[\"source_char_index\"][c] = idx\n                data[\"source_index_char\"][idx] = c\n        for c in padded_target_strings[i]:\n            if data[\"target_char_index\"].get(c) is None:\n                data[\"target_chars\"].append(c)\n                idx = len(data[\"target_chars\"]) - 1\n                data[\"target_char_index\"][c] = idx\n                data[\"target_index_char\"][idx] = c\n\n    # Generate sequences of indexes for source and target data\n    data['source_data_seq'] = generate_string_to_sequence(padded_source_strings, data['source_char_index'])\n    data['target_data_seq'] = generate_string_to_sequence(padded_target_strings, data['target_char_index'])\n    \n\n    # Update lengths of source and target character lists\n    data[\"source_len\"] = len(data[\"source_chars\"])\n    data[\"target_len\"] = len(data[\"target_chars\"])\n    \n    return data\n\n\n# Function to get the appropriate cell type based on the input string\ndef get_cell_type(cell_type):\n    if cell_type == \"RNN\":\n        return nn.RNN\n    elif cell_type == \"LSTM\":\n        return nn.LSTM\n    elif cell_type == \"GRU\":\n        return nn.GRU\n    else:\n        print(\"Specify correct cell type\")\n\n# Encoder module for the seq2seq model\nclass Encoder(nn.Module):\n    def __init__(self, h_params, data, device):\n        super(Encoder, self).__init__()\n        # Embedding layer for input data\n        self.embedding = nn.Embedding(data[\"source_len\"], h_params[\"char_embd_dim\"])\n        # Dropout layer for regularization\n        self.dropout = nn.Dropout(h_params[\"dropout\"])\n        # Cell type chosen for encoding\n        self.cell = get_cell_type(h_params[\"cell_type\"])(h_params[\"char_embd_dim\"], h_params[\"hidden_layer_neurons\"],\n                                                         num_layers=h_params[\"number_of_layers\"], dropout=h_params[\"dropout\"], batch_first=True)\n        self.device = device\n        self.h_params = h_params\n\n    def forward(self, current_input, prev_state):\n        # Embedding input\n        embd_input = self.embedding(current_input)\n        embd_input = self.dropout(embd_input)\n        # Encoding step\n        output, prev_state = self.cell(embd_input, prev_state)\n        return output, prev_state\n\n    # Initialize initial state of the encoder\n    def getInitialState(self):\n        return torch.zeros(self.h_params[\"number_of_layers\"], self.h_params[\"batch_size\"], self.h_params[\"hidden_layer_neurons\"], device=self.device)\n\n# Decoder module for the seq2seq model\nclass Decoder(nn.Module):\n    def __init__(self, h_params, data, device):\n        super(Decoder, self).__init__()\n        # Embedding layer for target data\n        self.embedding = nn.Embedding(data[\"target_len\"], h_params[\"char_embd_dim\"])\n        # Dropout layer for regularization\n        self.dropout = nn.Dropout(h_params[\"dropout\"])\n        # Cell type chosen for decoding\n        self.cell = get_cell_type(h_params[\"cell_type\"])(h_params[\"char_embd_dim\"], h_params[\"hidden_layer_neurons\"],\n                                                         num_layers=h_params[\"number_of_layers\"], dropout=h_params[\"dropout\"], batch_first=True)\n        # Fully connected layer for output\n        self.fc = nn.Linear(h_params[\"hidden_layer_neurons\"], data[\"target_len\"])\n        # Softmax layer for probability distribution\n        self.softmax = nn.LogSoftmax(dim=2)\n        self.h_params = h_params\n\n    def forward(self, current_input, prev_state):\n        # Embedding input\n        embd_input = self.embedding(current_input)\n        curr_embd = F.relu(embd_input)\n        curr_embd = self.dropout(curr_embd)\n        # Decoding step\n        output, prev_state = self.cell(curr_embd, prev_state)\n        # Apply softmax to output\n        output = self.softmax(self.fc(output))\n        return output, prev_state\n\n\n# Custom dataset class for handling source and target sequences\nclass MyDataset(Dataset):\n    def __init__(self, data):\n        # Initialize with source and target sequences\n        self.source_data_seq = data[0]\n        self.target_data_seq = data[1]\n    \n    def __len__(self):\n        # Return the length of the dataset\n        return len(self.source_data_seq)\n    \n    def __getitem__(self, idx):\n        # Get source and target data at the specified index\n        source_data = self.source_data_seq[idx]\n        target_data = self.target_data_seq[idx]\n        return source_data, target_data\n\n\n# Function for inference on the trained model\ndef inference(encoder, decoder, source_sequence, target_tensor, data, device, h_params, loss_fn, batch_num):\n    # Set encoder and decoder to evaluation mode\n    encoder.eval()\n    decoder.eval()\n    \n    # Initialize loss and correct predictions\n    loss = 0\n    correct = 0\n    \n    # Turn off gradient calculation\n    with torch.no_grad():\n        # Initialize encoder hidden state\n        encoder_hidden = encoder.getInitialState()\n        # For LSTM, initialize cell state as well\n        if h_params[\"cell_type\"] == \"LSTM\":\n            encoder_hidden = (encoder_hidden, encoder.getInitialState())\n        # Perform encoding\n        encoder_outputs, encoder_hidden = encoder(source_sequence, encoder_hidden)\n\n        # Initialize decoder input with start token\n        decoder_input_tensor = torch.full((h_params[\"batch_size\"], 1), data['target_char_index'][START_TOKEN], device=device)\n        decoder_actual_output = []\n        \n        # Initialize decoder hidden state with encoder hidden state\n        decoder_hidden = encoder_hidden\n        \n        # Iterate over each output time step\n        for di in range(data[\"OUTPUT_MAX_LENGTH\"]):\n            curr_target_chars = target_tensor[:, di]\n            # Perform decoding for one time step\n            decoder_output, decoder_hidden = decoder(decoder_input_tensor, decoder_hidden)\n            topv, topi = decoder_output.topk(1)\n            decoder_input_tensor = topi.squeeze().detach()\n            decoder_actual_output.append(decoder_input_tensor)\n            decoder_input_tensor = decoder_input_tensor.view(h_params[\"batch_size\"], 1)\n                        \n            decoder_output = decoder_output[:, -1, :]\n            # Compute loss for the current time step\n            loss += (loss_fn(decoder_output, curr_target_chars))\n\n        # Concatenate decoder outputs\n        decoder_actual_output = torch.cat(decoder_actual_output, dim=0).view(data[\"OUTPUT_MAX_LENGTH\"], h_params[\"batch_size\"]).transpose(0, 1)\n\n        # Compute number of correct predictions\n        correct = (decoder_actual_output == target_tensor).all(dim=1).sum().item()\n\n        return correct, loss.item() / data[\"OUTPUT_MAX_LENGTH\"]\n\n# Function to evaluate the model on validation or test data\ndef evaluate(encoder, decoder, data, dataloader, device, h_params, loss_fn):\n    correct_predictions = 0\n    total_loss = 0\n    total_predictions = len(dataloader.dataset)\n    number_of_batches = len(dataloader)\n    \n    # Iterate over each batch\n    for batch_num, (source_sequence, target_sequence) in enumerate(dataloader):\n        input_tensor = source_sequence\n        target_tensor = target_sequence\n        \n        # Perform inference on the batch\n        correct, loss = inference(encoder, decoder, input_tensor, target_tensor, data, device, h_params, loss_fn, batch_num)\n        \n        correct_predictions += correct\n        total_loss += loss\n    \n    # Compute accuracy and average loss\n    accuracy = correct_predictions / total_predictions\n    total_loss /= number_of_batches\n    \n    return accuracy, total_loss\n\n\n# Function to create strings from indexes\ndef make_strings(data, source, target, output):\n    source_string = \"\"\n    target_string = \"\"\n    output_string = \"\"\n    # Convert source indexes to characters\n    for i in source:\n        source_string += (data['source_index_char'][i.item()])\n    # Convert target indexes to characters\n    for i in target:\n        target_string += (data['target_index_char'][i.item()])\n    # Convert output indexes to characters\n    for i in output:\n        output_string += (data['target_index_char'][i.item()])\n    return source_string, target_string, output_string\n\n# Training loop\ndef train_loop(encoder, decoder, h_params, data, data_loader, val_dataloader, device):\n    # Choose optimizer based on the specified type\n    if h_params[\"optimizer\"] == \"adam\":\n        encoder_optimizer = optim.Adam(encoder.parameters(), lr=h_params[\"learning_rate\"])\n        decoder_optimizer = optim.Adam(decoder.parameters(), lr=h_params[\"learning_rate\"])\n    elif h_params[\"optimizer\"] == \"nadam\":\n        encoder_optimizer = optim.NAdam(encoder.parameters(), lr=h_params[\"learning_rate\"])\n        decoder_optimizer = optim.NAdam(decoder.parameters(), lr=h_params[\"learning_rate\"])\n    \n    total_predictions = len(data_loader.dataset)\n    total_batches = len(data_loader)\n    loss_fn = nn.NLLLoss()\n    \n    # Loop through each epoch\n    for ep in range(h_params[\"epochs\"]):\n        encoder.train()\n        decoder.train()\n        total_loss = 0\n        total_correct = 0\n        \n        # Loop through each batch\n        for batch_num, (source_batch, target_batch) in enumerate(data_loader):\n            # Get initial state of the encoder\n            encoder_initial_state = encoder.getInitialState()\n            if h_params[\"cell_type\"] == \"LSTM\":\n                encoder_initial_state = (encoder_initial_state, encoder.getInitialState())\n            \n            encoder_current_state = encoder_initial_state\n            encoder_output, encoder_current_state = encoder(source_batch, encoder_current_state)\n            loss = 0\n            correct = 0\n            decoder_curr_state = encoder_current_state\n            output_seq_len = data[\"OUTPUT_MAX_LENGTH\"]\n            decoder_actual_output = []\n            \n            # Determine whether to use teacher forcing\n            use_teacher_forcing = True if random.random() < TEACHER_FORCING_RATIO else False\n\n            # Loop through each output time step\n            for i in range(data[\"OUTPUT_MAX_LENGTH\"]):\n                if(i == 0):\n                    decoder_input_tensor = target_batch[:, i].view(h_params[\"batch_size\"], 1)\n                curr_target_chars = target_batch[:, i]\n                decoder_output, decoder_curr_state = decoder(decoder_input_tensor, decoder_curr_state)\n                topv, topi = decoder_output.topk(1)\n                decoder_input_tensor = topi.squeeze().detach()\n                decoder_actual_output.append(decoder_input_tensor)\n                if(i < output_seq_len - 1):\n                    if use_teacher_forcing:\n                        decoder_input_tensor = target_batch[:, i + 1].view(h_params[\"batch_size\"], 1)\n                    else:\n                        decoder_input_tensor = decoder_input_tensor.view(h_params[\"batch_size\"], 1)\n                decoder_output = decoder_output[:, -1, :]\n                loss += (loss_fn(decoder_output, curr_target_chars))\n\n            decoder_actual_output = torch.cat(decoder_actual_output, dim=0).view(output_seq_len, h_params[\"batch_size\"]).transpose(0, 1)\n            \n            correct = (decoder_actual_output == target_batch).all(dim=1).sum().item()\n            total_correct += correct\n            total_loss += loss.item() / output_seq_len\n            \n            # Backpropagation and optimization step\n            encoder_optimizer.zero_grad()\n            decoder_optimizer.zero_grad()\n            loss.backward()\n            encoder_optimizer.step()\n            decoder_optimizer.step()\n        \n        train_acc = total_correct / total_predictions\n        train_loss = total_loss / total_batches\n        \n        # Evaluate on validation set\n        val_acc, val_loss = evaluate(encoder, decoder, data, val_dataloader, device, h_params, loss_fn)\n        \n        # Log metrics\n        print(\"ep: \", ep, \" train acc:\", train_acc, \" train loss:\", train_loss, \" val acc:\", val_acc, \" val loss:\", val_loss)\n        wandb.log({\"train_accuracy\": train_acc, \"train_loss\": train_loss, \"val_accuracy\": val_acc, \"val_loss\": val_loss, \"epoch\": ep})\n\n    return encoder, decoder, loss_fn\n\n\n# Training function\ndef train(h_params, data, device, train_dataloader, val_dataloader):\n    # Initialize encoder and decoder\n    encoder = Encoder(h_params, data, device).to(device)\n    decoder = Decoder(h_params, data, device).to(device)\n    \n    # Perform training loop\n    encoder, decoder, loss_fn = train_loop(encoder, decoder, h_params, data, train_dataloader, val_dataloader, device)\n    return encoder, decoder, loss_fn\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-10T15:04:04.869803Z","iopub.execute_input":"2025-05-10T15:04:04.870077Z","iopub.status.idle":"2025-05-10T15:04:18.195072Z","shell.execute_reply.started":"2025-05-10T15:04:04.870055Z","shell.execute_reply":"2025-05-10T15:04:18.193963Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs23m059\u001b[0m (\u001b[33mcs23m059-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"name":"stdout","text":"cuda\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/1529598478.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Load the training data from the CSV file into a DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_csv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# Separate the source and target sequences from the training DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/d/cs23m059/telugu/te_train.csv'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/d/cs23m059/telugu/te_train.csv'","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"h_params = {\n    \"char_embd_dim\": 32,\n    \"hidden_layer_neurons\": 32,\n    \"batch_size\": 32,\n    \"number_of_layers\": 2,\n    \"learning_rate\": 0.0001,\n    \"epochs\": 20,\n    \"cell_type\": \"RNN\",\n    \"dropout\": 0,\n    \"optimizer\": \"adam\"\n}\n\n# Function to prepare dataloaders for training and validation\ndef prepare_dataloaders(train_source, train_target, val_source, val_target, h_params):\n    # Preprocess training data\n    data = preprocess_data(copy.copy(train_source), copy.copy(train_target))\n    training_data = [data[\"source_data_seq\"], data['target_data_seq']]\n    # Create training dataset and dataloader\n    train_dataset = MyDataset(training_data)\n    train_dataloader = DataLoader(train_dataset, batch_size=h_params[\"batch_size\"], shuffle=True)\n\n    # Preprocess validation data\n    val_padded_source_strings = add_padding(val_source, data[\"INPUT_MAX_LENGTH\"])\n    val_padded_target_strings = add_padding(val_target, data[\"OUTPUT_MAX_LENGTH\"])\n    val_source_sequences = generate_string_to_sequence(val_padded_source_strings, data['source_char_index'])\n    val_target_sequences = generate_string_to_sequence(val_padded_target_strings, data['target_char_index'])\n    validation_data = [val_source_sequences, val_target_sequences]\n    # Create validation dataset and dataloader\n    val_dataset = MyDataset(validation_data)\n    val_dataloader = DataLoader(val_dataset, batch_size=h_params[\"batch_size\"], shuffle=True)\n    return train_dataloader, val_dataloader, data\n\n\nconfig = h_params\n# run = wandb.init(project=\"DL Assignment 3\", name=f\"{config['cell_type']}_{config['optimizer']}_ep_{config['epochs']}_lr_{config['learning_rate']}_embd_{config['char_embd_dim']}_hid_lyr_neur_{config['hidden_layer_neurons']}_bs_{config['batch_size']}_enc_layers_{config['number_of_layers']}_dec_layers_{config['number_of_layers']}_dropout_{config['dropout']}\", config=config)\ntrain_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, h_params)\ntrain(h_params, data, device, train_dataloader, val_dataloader)\n\n# #Run this cell to run a sweep with appropriate parameters\n# sweep_params = {\n#     'method' : 'bayes',\n#     'name'   : 'DL assignment 3 sweep',\n#     'metric' : {\n#         'goal' : 'maximize',\n#         'name' : 'val_accuracy',\n#     },\n#     'parameters' : {\n#         'epochs':{'values' : [15, 20]},\n#         'learning_rate':{'values' : [0.001, 0.0001]},\n#         'batch_size':{'values':[32,64, 128]},\n#         'char_embd_dim':{'values' : [64, 128, 256] } ,\n#         'number_of_layers':{'values' : [1,2,3,4]},\n#         'optimizer':{'values':['nadam','adam']},\n#         'cell_type':{'values' : [\"RNN\",\"LSTM\", \"GRU\"]},\n#         'hidden_layer_neurons':{'values': [ 128, 256, 512]},\n#         'dropout':{'values': [0,0.2, 0.3]}\n#     }\n# }\n\n# sweep_id = wandb.sweep(sweep=sweep_params, project=\"DL Assignment 3\")\n# def main():\n#     wandb.init(project=\"DL Assignment 3\" )\n#     config = wandb.config\n#     with wandb.init(project=\"DL Assignment 3\", name=f\"{config['cell_type']}_{config['optimizer']}_ep_{config['epochs']}_lr_{config['learning_rate']}_embd_{config['char_embd_dim']}_hid_lyr_neur_{config['hidden_layer_neurons']}_bs_{config['batch_size']}_enc_layers_{config['number_of_layers']}_dec_layers_{config['number_of_layers']}_dropout_{config['dropout']}\", config=config):\n#         train_dataloader, val_dataloader, data = prepare_dataloaders(train_source, train_target, val_source, val_target, config)\n#         train(config, data, device, train_dataloader, val_dataloader)\n\n\n# wandb.agent(\"hw3b5jng\", function=main, count=100)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T15:04:18.195636Z","iopub.status.idle":"2025-05-10T15:04:18.195912Z","shell.execute_reply.started":"2025-05-10T15:04:18.195766Z","shell.execute_reply":"2025-05-10T15:04:18.195783Z"}},"outputs":[],"execution_count":null}]}