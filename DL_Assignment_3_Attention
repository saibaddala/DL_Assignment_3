{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11763850,"sourceType":"datasetVersion","datasetId":7385249}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport pandas as pd\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport copy\nfrom torch.utils.data import Dataset, DataLoader\nimport random\nimport wandb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T05:19:41.638139Z","iopub.execute_input":"2025-05-11T05:19:41.639053Z","iopub.status.idle":"2025-05-11T05:19:45.986561Z","shell.execute_reply.started":"2025-05-11T05:19:41.639021Z","shell.execute_reply":"2025-05-11T05:19:45.985980Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"wandb.login(key=\"3117f688d100f7889a8f97ba664299887fe48de1\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T05:19:48.070658Z","iopub.execute_input":"2025-05-11T05:19:48.071203Z","iopub.status.idle":"2025-05-11T05:19:53.741044Z","shell.execute_reply.started":"2025-05-11T05:19:48.071180Z","shell.execute_reply":"2025-05-11T05:19:53.740498Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs23m059\u001b[0m (\u001b[33mcs23m059-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T05:19:55.138844Z","iopub.execute_input":"2025-05-11T05:19:55.139613Z","iopub.status.idle":"2025-05-11T05:19:55.200121Z","shell.execute_reply.started":"2025-05-11T05:19:55.139589Z","shell.execute_reply":"2025-05-11T05:19:55.199302Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# File paths\ntrain_csv = \"/kaggle/input/telugu/te_train.csv\"\ntest_csv = \"/kaggle/input/telugu/te_test.csv\"\nval_csv = \"/kaggle/input/telugu/te_val.csv\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T05:19:55.890964Z","iopub.execute_input":"2025-05-11T05:19:55.891230Z","iopub.status.idle":"2025-05-11T05:19:55.894997Z","shell.execute_reply.started":"2025-05-11T05:19:55.891211Z","shell.execute_reply":"2025-05-11T05:19:55.894315Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"train_data = pd.read_csv(train_csv, header=None)\ntrain_input = train_data[0].to_numpy()\ntrain_output = train_data[1].to_numpy()\nval_data = pd.read_csv(val_csv, header=None)\nval_input = val_data[0].to_numpy()\nval_output = val_data[1].to_numpy()\ntest_data = pd.read_csv(test_csv, header=None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T05:19:56.794522Z","iopub.execute_input":"2025-05-11T05:19:56.794790Z","iopub.status.idle":"2025-05-11T05:19:56.921401Z","shell.execute_reply.started":"2025-05-11T05:19:56.794770Z","shell.execute_reply":"2025-05-11T05:19:56.920763Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def pre_processing(train_input, train_output):\n    data = {\n        \"all_characters\": [],\n        \"char_num_map\": {},\n        \"num_char_map\": {},\n        \"source_charToNum\": torch.zeros(len(train_input), 30, dtype=torch.int, device=device),\n        \"source_data\": train_input,\n        \"all_characters_2\": [],\n        \"char_num_map_2\": {},\n        \"num_char_map_2\": {},\n        \"val_charToNum\": torch.zeros(len(train_output), 23, dtype=torch.int, device=device),\n        \"target_data\": train_output,\n        \"source_len\": 0,\n        \"target_len\": 0\n    }\n    k = 0\n    for i in range(len(train_input)):\n        train_input[i] = \"{\" + train_input[i] + \"}\" * (29 - len(train_input[i]))\n        charToNum = []\n        for char in train_input[i]:\n            if char not in data[\"all_characters\"]:\n                data[\"all_characters\"].append(char)\n                index = data[\"all_characters\"].index(char)\n                data[\"char_num_map\"][char] = index\n                data[\"num_char_map\"][index] = char\n            else:\n                index = data[\"all_characters\"].index(char)\n            charToNum.append(index)\n        my_tensor = torch.tensor(charToNum, device=device)\n        data[\"source_charToNum\"][k] = my_tensor\n\n        charToNum1 = []\n        train_output[i] = \"{\" + train_output[i] + \"}\" * (22 - len(train_output[i]))\n        for char in train_output[i]:\n            if char not in data[\"all_characters_2\"]:\n                data[\"all_characters_2\"].append(char)\n                index = data[\"all_characters_2\"].index(char)\n                data[\"char_num_map_2\"][char] = index\n                data[\"num_char_map_2\"][index] = char\n            else:\n                index = data[\"all_characters_2\"].index(char)\n            charToNum1.append(index)\n        my_tensor1 = torch.tensor(charToNum1, device=device)\n        data[\"val_charToNum\"][k] = my_tensor1\n        k += 1\n\n    data[\"source_len\"] = len(data[\"all_characters\"])\n    data[\"target_len\"] = len(data[\"all_characters_2\"])\n    return data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T05:19:57.707528Z","iopub.execute_input":"2025-05-11T05:19:57.708079Z","iopub.status.idle":"2025-05-11T05:19:57.717132Z","shell.execute_reply.started":"2025-05-11T05:19:57.708056Z","shell.execute_reply":"2025-05-11T05:19:57.716499Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"data = pre_processing(copy.copy(train_input), copy.copy(train_output))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T05:20:00.050648Z","iopub.execute_input":"2025-05-11T05:20:00.051188Z","iopub.status.idle":"2025-05-11T05:20:06.023118Z","shell.execute_reply.started":"2025-05-11T05:20:00.051165Z","shell.execute_reply":"2025-05-11T05:20:06.022393Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def pre_processing_validation(val_input, val_output):\n    data2 = {\n        \"all_characters\": [],\n        \"char_num_map\": {},\n        \"num_char_map\": {},\n        \"source_charToNum\": torch.zeros(len(val_input), 30, dtype=torch.int, device=device),\n        \"source_data\": val_input,\n        \"all_characters_2\": [],\n        \"char_num_map_2\": {},\n        \"num_char_map_2\": {},\n        \"val_charToNum\": torch.zeros(len(val_output), 23, dtype=torch.int, device=device),\n        \"target_data\": val_output,\n        \"source_len\": 0,\n        \"target_len\": 0\n    }\n    k = 0\n    m1 = data[\"char_num_map\"]\n    m2 = data[\"char_num_map_2\"]\n    for i in range(len(val_input)):\n        val_input[i] = \"{\" + val_input[i] + \"}\" * (29 - len(val_input[i]))\n        charToNum = []\n        for char in val_input[i]:\n            if char not in data2[\"all_characters\"]:\n                data2[\"all_characters\"].append(char)\n                index = m1[char]\n                data2[\"char_num_map\"][char] = index\n                data2[\"num_char_map\"][index] = char\n            else:\n                index = m1[char]\n            charToNum.append(index)\n        my_tensor = torch.tensor(charToNum, device=device)\n        data2[\"source_charToNum\"][k] = my_tensor\n\n        charToNum1 = []\n        val_output[i] = \"{\" + val_output[i] + \"}\" * (22 - len(val_output[i]))\n        for char in val_output[i]:\n            if char not in data2[\"all_characters_2\"]:\n                data2[\"all_characters_2\"].append(char)\n                index = m2[char]\n                data2[\"char_num_map_2\"][char] = index\n                data2[\"num_char_map_2\"][index] = char\n            else:\n                index = m2[char]\n            charToNum1.append(index)\n        my_tensor1 = torch.tensor(charToNum1, device=device)\n        data2[\"val_charToNum\"][k] = my_tensor1\n        k += 1\n\n    data2[\"source_len\"] = len(data2[\"all_characters\"])\n    data2[\"target_len\"] = len(data2[\"all_characters_2\"])\n    return data2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T05:20:07.132403Z","iopub.execute_input":"2025-05-11T05:20:07.132941Z","iopub.status.idle":"2025-05-11T05:20:07.140712Z","shell.execute_reply.started":"2025-05-11T05:20:07.132919Z","shell.execute_reply":"2025-05-11T05:20:07.140087Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"data2 = pre_processing_validation(copy.copy(val_input), copy.copy(val_output))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T05:20:09.819926Z","iopub.execute_input":"2025-05-11T05:20:09.820638Z","iopub.status.idle":"2025-05-11T05:20:10.325943Z","shell.execute_reply.started":"2025-05-11T05:20:09.820612Z","shell.execute_reply":"2025-05-11T05:20:10.325377Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class MyDataset(Dataset):\n    def __init__(self, x, y):\n        self.source = x\n        self.target = y\n\n    def __len__(self):\n        return len(self.source)\n\n    def __getitem__(self, idx):\n        source_data = self.source[idx]\n        target_data = self.target[idx]\n        return source_data, target_data\n\ndef validationAccuracy(encoder, decoder, batchsize, tf_ratio, cellType, bidirection):\n    dataLoader = dataLoaderFun(\"validation\", batchsize)\n    encoder.eval()\n    decoder.eval()\n    total_sequences = 0\n    total_correct_sequences = 0\n    total_char_matches = 0\n    total_characters = 0\n    total_loss = 0\n\n    lossFunction = nn.NLLLoss()\n\n    for source_batch, target_batch in dataLoader:\n        actual_batch_size = source_batch.shape[0]\n        total_sequences += actual_batch_size\n        total_characters += target_batch.numel()\n\n        encoder_initial_state = encoder.getInitialState(actual_batch_size)\n        if bidirection == \"Yes\":\n            reversed_batch = torch.flip(source_batch, dims=[1])\n            source_batch = (source_batch + reversed_batch) // 2\n        if cellType == 'LSTM':\n            encoder_initial_state = (encoder_initial_state, encoder.getInitialState(actual_batch_size))\n\n        encoder_states, _ = encoder(source_batch, encoder_initial_state)\n        decoder_current_state = encoder_states[-1, :, :, :]\n        encoder_final_layer_states = encoder_states[:, -1, :, :]\n        output_seq_len = target_batch.shape[1]\n\n        loss = 0\n        decoder_actual_output = []\n        randNumber = random.random()\n\n        for i in range(output_seq_len):\n            if i == 0:\n                decoder_current_input = torch.full((actual_batch_size, 1), 0, device=device)\n            else:\n                if randNumber < tf_ratio:\n                    decoder_current_input = target_batch[:, i].reshape(actual_batch_size, 1)\n                else:\n                    decoder_current_input = decoder_current_input.reshape(actual_batch_size, 1)\n            decoder_output, decoder_current_state, _ = decoder(decoder_current_input, decoder_current_state, encoder_final_layer_states)\n            topv, topi = decoder_output.topk(1)\n            decoder_current_input = topi.squeeze().detach()\n            decoder_actual_output.append(decoder_current_input)\n\n            decoder_output = decoder_output[:, -1, :]\n            curr_target_chars = target_batch[:, i].long()\n            loss += lossFunction(decoder_output, curr_target_chars)\n\n        total_loss += loss.item() / output_seq_len\n        decoder_actual_output = torch.cat(decoder_actual_output, dim=0).reshape(output_seq_len, actual_batch_size).transpose(0, 1)\n        total_correct_sequences += (decoder_actual_output == target_batch).all(dim=1).sum().item()\n        total_char_matches += (decoder_actual_output == target_batch).sum().item()\n\n    encoder.train()\n    decoder.train()\n\n    wandb.log({\n        'validation_loss': total_loss / len(dataLoader),\n        'validation_accuracy': total_correct_sequences / total_sequences,\n        'validation_char_accuracy': total_char_matches / total_characters\n    })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T05:20:12.156445Z","iopub.execute_input":"2025-05-11T05:20:12.156925Z","iopub.status.idle":"2025-05-11T05:20:12.166722Z","shell.execute_reply.started":"2025-05-11T05:20:12.156900Z","shell.execute_reply":"2025-05-11T05:20:12.165892Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self, hidden_size):\n        super(Attention, self).__init__()\n        self.Wa = nn.Linear(hidden_size, hidden_size)\n        self.Ua = nn.Linear(hidden_size, hidden_size)\n        self.Va = nn.Linear(hidden_size, 1)\n\n    def forward(self, query, keys):\n        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n        scores = scores.squeeze().unsqueeze(1)\n        weights = F.softmax(scores, dim=0)\n        weights = weights.permute(2, 1, 0)\n        keys = keys.permute(1, 0, 2)\n        context = torch.bmm(weights, keys)\n        return context, weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T05:20:13.835672Z","iopub.execute_input":"2025-05-11T05:20:13.836147Z","iopub.status.idle":"2025-05-11T05:20:13.841104Z","shell.execute_reply.started":"2025-05-11T05:20:13.836124Z","shell.execute_reply":"2025-05-11T05:20:13.840530Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, inputDim, embSize, encoderLayers, hiddenLayerNuerons, cellType, batch_size):\n        super(Encoder, self).__init__()\n        self.embedding = nn.Embedding(inputDim, embSize)\n        self.encoderLayers = encoderLayers\n        self.hiddenLayerNuerons = hiddenLayerNuerons\n        self.cellType = cellType\n        if cellType == 'GRU':\n            self.rnn = nn.GRU(embSize, hiddenLayerNuerons, num_layers=encoderLayers, batch_first=True)\n        elif cellType == 'RNN':\n            self.rnn = nn.RNN(embSize, hiddenLayerNuerons, num_layers=encoderLayers, batch_first=True)\n        else:\n            self.rnn = nn.LSTM(embSize, hiddenLayerNuerons, num_layers=encoderLayers, batch_first=True)\n\n    def forward(self, sourceBatch, encoderCurrState):\n        sequenceLength = sourceBatch.shape[1]\n        batch_size = sourceBatch.shape[0]\n        encoderStates = torch.zeros(sequenceLength, self.encoderLayers, batch_size, self.hiddenLayerNuerons, device=device)\n        for i in range(sequenceLength):\n            currInput = sourceBatch[:, i].reshape(batch_size, 1)\n            _, encoderCurrState = self.statesCalculation(currInput, encoderCurrState)\n            if self.cellType == 'LSTM':\n                encoderStates[i] = encoderCurrState[1]\n            else:\n                encoderStates[i] = encoderCurrState\n        return encoderStates, encoderCurrState\n\n    def statesCalculation(self, currentInput, prevState):\n        embdInput = self.embedding(currentInput)\n        output, prev_state = self.rnn(embdInput, prevState)\n        return output, prev_state\n\n    def getInitialState(self, batch_size):\n        return torch.zeros(self.encoderLayers, batch_size, self.hiddenLayerNuerons, device=device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T05:20:15.246070Z","iopub.execute_input":"2025-05-11T05:20:15.246571Z","iopub.status.idle":"2025-05-11T05:20:15.253464Z","shell.execute_reply.started":"2025-05-11T05:20:15.246547Z","shell.execute_reply":"2025-05-11T05:20:15.252712Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, outputDim, embSize, hiddenLayerNuerons, decoderLayers, cellType, dropout_p):\n        super(Decoder, self).__init__()\n        self.embedding = nn.Embedding(outputDim, embSize)\n        self.cellType = cellType\n        if cellType == 'GRU':\n            self.rnn = nn.GRU(embSize + hiddenLayerNuerons, hiddenLayerNuerons, num_layers=decoderLayers, batch_first=True)\n        elif cellType == 'RNN':\n            self.rnn = nn.RNN(embSize + hiddenLayerNuerons, hiddenLayerNuerons, num_layers=decoderLayers, batch_first=True)\n        else:\n            self.rnn = nn.LSTM(embSize + hiddenLayerNuerons, hiddenLayerNuerons, num_layers=decoderLayers, batch_first=True)\n        self.fc = nn.Linear(hiddenLayerNuerons, outputDim)\n        self.softmax = nn.LogSoftmax(dim=2)\n        self.dropout = nn.Dropout(dropout_p)\n        self.attention = Attention(hiddenLayerNuerons).to(device)\n\n    def forward(self, current_input, prev_state, encoder_final_layers):\n        if self.cellType == 'LSTM':\n            context, attn_weights = self.attention(prev_state[1][-1, :, :], encoder_final_layers)\n        else:\n            context, attn_weights = self.attention(prev_state[-1, :, :], encoder_final_layers)\n        embd_input = self.embedding(current_input)\n        curr_embd = F.relu(embd_input)\n        input_gru = torch.cat((curr_embd, context), dim=2)\n        output, prev_state = self.rnn(input_gru, prev_state)\n        output = self.dropout(output)\n        output = self.softmax(self.fc(output))\n        return output, prev_state, attn_weights\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T05:20:16.747911Z","iopub.execute_input":"2025-05-11T05:20:16.748536Z","iopub.status.idle":"2025-05-11T05:20:16.755126Z","shell.execute_reply.started":"2025-05-11T05:20:16.748512Z","shell.execute_reply":"2025-05-11T05:20:16.754427Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def dataLoaderFun(dataName, batch_size):\n    if dataName == 'train':\n        dataset = MyDataset(data[\"source_charToNum\"], data['val_charToNum'])\n        return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n    else:\n        dataset = MyDataset(data2[\"source_charToNum\"], data2['val_charToNum'])\n        return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\ndef train(embSize, encoderLayers, decoderLayers, hiddenLayerNuerons, cellType, bidirection, dropout, epochs, batchsize, learningRate, optimizer, tf_ratio):\n    dataLoader = dataLoaderFun(\"train\", batchsize)\n    encoder = Encoder(data[\"source_len\"], embSize, encoderLayers, hiddenLayerNuerons, cellType, batchsize).to(device)\n    decoder = Decoder(data[\"target_len\"], embSize, hiddenLayerNuerons, encoderLayers, cellType, dropout).to(device)\n    if optimizer == 'Adam':\n        encoderOptimizer = optim.Adam(encoder.parameters(), lr=learningRate)\n        decoderOptimizer = optim.Adam(decoder.parameters(), lr=learningRate)\n    else:\n        encoderOptimizer = optim.NAdam(encoder.parameters(), lr=learningRate)\n        decoderOptimizer = optim.NAdam(decoder.parameters(), lr=learningRate)\n    lossFunction = nn.NLLLoss()\n    for epoch in range(epochs):\n        train_accuracy = 0\n        train_loss = 0\n        for batch_num, (source_batch, target_batch) in enumerate(dataLoader):\n            actual_batch_size = source_batch.shape[0]\n            encoder_initial_state = encoder.getInitialState(actual_batch_size)\n            if bidirection == \"Yes\":\n                reversed_batch = torch.flip(source_batch, dims=[1])\n                source_batch = (source_batch + reversed_batch) // 2\n            if cellType == 'LSTM':\n                encoder_initial_state = (encoder_initial_state, encoder.getInitialState(actual_batch_size))\n            encoder_states, dummy = encoder(source_batch, encoder_initial_state)\n            decoder_current_state = dummy\n            encoder_final_layer_states = encoder_states[:, -1, :, :]\n            loss = 0\n            output_seq_len = target_batch.shape[1]\n            decoder_actual_output = []\n            randNumber = random.random()\n            for i in range(output_seq_len):\n                if i == 0:\n                    decoder_current_input = torch.full((actual_batch_size, 1), 0, device=device)\n                else:\n                    if randNumber < tf_ratio:\n                        decoder_current_input = target_batch[:, i].reshape(actual_batch_size, 1)\n                    else:\n                        decoder_current_input = decoder_current_input.reshape(actual_batch_size, 1)\n                decoder_output, decoder_current_state, _ = decoder(decoder_current_input, decoder_current_state, encoder_final_layer_states)\n                topv, topi = decoder_output.topk(1)\n                decoder_current_input = topi.squeeze().detach()\n                decoder_actual_output.append(decoder_current_input)\n                decoder_output = decoder_output[:, -1, :]\n                curr_target_chars = target_batch[:, i].type(dtype=torch.long)\n                loss += (lossFunction(decoder_output, curr_target_chars))\n            decoder_actual_output = torch.cat(decoder_actual_output, dim=0).reshape(output_seq_len, actual_batch_size).transpose(0, 1)\n            train_accuracy += (decoder_actual_output == target_batch).all(dim=1).sum().item()\n            train_loss += (loss.item() / output_seq_len)\n            encoderOptimizer.zero_grad()\n            decoderOptimizer.zero_grad()\n            loss.backward()\n            encoderOptimizer.step()\n            decoderOptimizer.step()\n\n        # ✅ Logging train metrics here\n        wandb.log({'train_accuracy': train_accuracy / len(data[\"source_charToNum\"])})\n        wandb.log({'train_loss': train_loss / len(dataLoader)})\n\n        validationAccuracy(encoder, decoder, batchsize, tf_ratio, cellType, bidirection)\n\n\ndef numToCharConverter(inputArray, outputArray, data):\n    mp = data['num_char_map_2']\n    for row1, row2 in zip(inputArray, outputArray):\n        t1 = ''.join([mp[e1.item()] for e1 in row1])\n        t2 = ''.join([mp[e2.item()] for e2 in row2])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T05:20:18.460280Z","iopub.execute_input":"2025-05-11T05:20:18.460857Z","iopub.status.idle":"2025-05-11T05:20:18.474000Z","shell.execute_reply.started":"2025-05-11T05:20:18.460834Z","shell.execute_reply":"2025-05-11T05:20:18.473358Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def main_fun():\n    wandb.init(project='DA6401_Assignment_3')\n    params = wandb.config\n    with wandb.init(project='DA6401_Assignment_3', name='embedding'+str(params.embSize)+'cellType'+params.cellType+'batchSize'+str(params.batchsize)) as run:\n        train(params.embSize, params.encoderLayers, params.decoderLayers, params.hiddenLayerNuerons, params.cellType, params.bidirection, params.dropout, params.epochs, params.batchsize, params.learningRate, params.optimizer, params.tf_ratio)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T05:20:21.988713Z","iopub.execute_input":"2025-05-11T05:20:21.988976Z","iopub.status.idle":"2025-05-11T05:20:21.994168Z","shell.execute_reply.started":"2025-05-11T05:20:21.988958Z","shell.execute_reply":"2025-05-11T05:20:21.993573Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"sweep_params = {\n    'method': 'bayes',\n    'name': 'DA6401_Assignment_3',\n    'metric': {\n        'goal': 'maximize',\n        'name': 'validation_accuracy',\n    },\n    'parameters': {\n        'embSize': {'values': [16, 32, 64]},\n        'encoderLayers': {'values': [1, 5, 10]},\n        'decoderLayers': {'values': [1, 5, 10]},\n        'hiddenLayerNuerons': {'values': [64, 256, 512]},\n        'cellType': {'values': ['GRU', 'RNN']},\n        'bidirection': {'values': ['no', 'Yes']},\n        'dropout': {'values': [0, 0.2, 0.3]},\n        'epochs': {'values': [10, 15]},\n        'batchsize': {'values': [32, 64]},\n        'learningRate': {'values': [1e-2, 1e-3, 1e-4]},\n        'optimizer': {'values': ['Adam', 'Nadam']},\n        'tf_ratio': {'values': [0.2, 0.4, 0.5]}\n    }\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T05:20:24.172065Z","iopub.execute_input":"2025-05-11T05:20:24.172356Z","iopub.status.idle":"2025-05-11T05:20:24.177828Z","shell.execute_reply.started":"2025-05-11T05:20:24.172336Z","shell.execute_reply":"2025-05-11T05:20:24.176966Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"sweepId = wandb.sweep(sweep_params, project='DA6401_Assignment_3')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T21:26:32.390208Z","iopub.execute_input":"2025-05-10T21:26:32.390449Z","iopub.status.idle":"2025-05-10T21:26:32.960441Z","shell.execute_reply.started":"2025-05-10T21:26:32.390426Z","shell.execute_reply":"2025-05-10T21:26:32.959807Z"}},"outputs":[{"name":"stdout","text":"Create sweep with ID: 4i8njfsm\nSweep URL: https://wandb.ai/cs23m059-iit-madras/DA6401_Assignment_3/sweeps/4i8njfsm\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"wandb.agent(\"4i8njfsm\", function=main_fun, count=30, entity=\"cs23m059-iit-madras\", project=\"DA6401_Assignment_3\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T05:24:06.068141Z","iopub.execute_input":"2025-05-11T05:24:06.068774Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mi6rw7y7 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchsize: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirection: no\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcellType: RNN\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoderLayers: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembSize: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tencoderLayers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \thiddenLayerNuerons: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearningRate: 0.0001\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Adam\n\u001b[34m\u001b[1mwandb\u001b[0m: \ttf_ratio: 0.4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'DA6401_Assignment_3' when running a sweep."},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250511_052412-mi6rw7y7</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cs23m059-iit-madras/DA6401_Assignment_3/runs/mi6rw7y7' target=\"_blank\">deft-sweep-10</a></strong> to <a href='https://wandb.ai/cs23m059-iit-madras/DA6401_Assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23m059-iit-madras/DA6401_Assignment_3/sweeps/4i8njfsm' target=\"_blank\">https://wandb.ai/cs23m059-iit-madras/DA6401_Assignment_3/sweeps/4i8njfsm</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cs23m059-iit-madras/DA6401_Assignment_3' target=\"_blank\">https://wandb.ai/cs23m059-iit-madras/DA6401_Assignment_3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/cs23m059-iit-madras/DA6401_Assignment_3/sweeps/4i8njfsm' target=\"_blank\">https://wandb.ai/cs23m059-iit-madras/DA6401_Assignment_3/sweeps/4i8njfsm</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cs23m059-iit-madras/DA6401_Assignment_3/runs/mi6rw7y7' target=\"_blank\">https://wandb.ai/cs23m059-iit-madras/DA6401_Assignment_3/runs/mi6rw7y7</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'DA6401_Assignment_3' when running a sweep."},"metadata":{}}],"execution_count":null}]}