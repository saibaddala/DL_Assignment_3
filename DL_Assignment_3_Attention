{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11763850,"sourceType":"datasetVersion","datasetId":7385249}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport pandas as pd\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport copy\nfrom torch.utils.data import Dataset, DataLoader\nimport random\nimport wandb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T01:53:16.008345Z","iopub.execute_input":"2025-05-12T01:53:16.008906Z","iopub.status.idle":"2025-05-12T01:53:22.567490Z","shell.execute_reply.started":"2025-05-12T01:53:16.008865Z","shell.execute_reply":"2025-05-12T01:53:22.566921Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"wandb.login(key=\"3117f688d100f7889a8f97ba664299887fe48de1\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T01:53:22.568684Z","iopub.execute_input":"2025-05-12T01:53:22.568921Z","iopub.status.idle":"2025-05-12T01:53:28.414874Z","shell.execute_reply.started":"2025-05-12T01:53:22.568896Z","shell.execute_reply":"2025-05-12T01:53:28.414252Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs23m059\u001b[0m (\u001b[33mcs23m059-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T01:53:28.415475Z","iopub.execute_input":"2025-05-12T01:53:28.415787Z","iopub.status.idle":"2025-05-12T01:53:28.470451Z","shell.execute_reply.started":"2025-05-12T01:53:28.415770Z","shell.execute_reply":"2025-05-12T01:53:28.469741Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# File paths\ntrain_csv = \"/kaggle/input/telugu/te_train.csv\"\ntest_csv = \"/kaggle/input/telugu/te_test.csv\"\nval_csv = \"/kaggle/input/telugu/te_val.csv\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T01:53:28.472073Z","iopub.execute_input":"2025-05-12T01:53:28.472271Z","iopub.status.idle":"2025-05-12T01:53:28.484475Z","shell.execute_reply.started":"2025-05-12T01:53:28.472255Z","shell.execute_reply":"2025-05-12T01:53:28.483864Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"train_data = pd.read_csv(train_csv, header=None)\ntrain_input = train_data[0].to_numpy()\ntrain_output = train_data[1].to_numpy()\nval_data = pd.read_csv(val_csv, header=None)\nval_input = val_data[0].to_numpy()\nval_output = val_data[1].to_numpy()\ntest_data = pd.read_csv(test_csv, header=None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T01:53:28.485150Z","iopub.execute_input":"2025-05-12T01:53:28.485969Z","iopub.status.idle":"2025-05-12T01:53:28.636059Z","shell.execute_reply.started":"2025-05-12T01:53:28.485944Z","shell.execute_reply":"2025-05-12T01:53:28.635515Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def pre_processing(train_input, train_output):\n    data = {\n        \"all_characters\": [],\n        \"char_num_map\": {},\n        \"num_char_map\": {},\n        \"source_charToNum\": torch.zeros(len(train_input), 30, dtype=torch.int, device=device),\n        \"source_data\": train_input,\n        \"all_characters_2\": [],\n        \"char_num_map_2\": {},\n        \"num_char_map_2\": {},\n        \"val_charToNum\": torch.zeros(len(train_output), 23, dtype=torch.int, device=device),\n        \"target_data\": train_output,\n        \"source_len\": 0,\n        \"target_len\": 0\n    }\n    k = 0\n    for i in range(len(train_input)):\n        train_input[i] = \"{\" + train_input[i] + \"}\" * (29 - len(train_input[i]))\n        charToNum = []\n        for char in train_input[i]:\n            if char not in data[\"all_characters\"]:\n                data[\"all_characters\"].append(char)\n                index = data[\"all_characters\"].index(char)\n                data[\"char_num_map\"][char] = index\n                data[\"num_char_map\"][index] = char\n            else:\n                index = data[\"all_characters\"].index(char)\n            charToNum.append(index)\n        my_tensor = torch.tensor(charToNum, device=device)\n        data[\"source_charToNum\"][k] = my_tensor\n\n        charToNum1 = []\n        train_output[i] = \"{\" + train_output[i] + \"}\" * (22 - len(train_output[i]))\n        for char in train_output[i]:\n            if char not in data[\"all_characters_2\"]:\n                data[\"all_characters_2\"].append(char)\n                index = data[\"all_characters_2\"].index(char)\n                data[\"char_num_map_2\"][char] = index\n                data[\"num_char_map_2\"][index] = char\n            else:\n                index = data[\"all_characters_2\"].index(char)\n            charToNum1.append(index)\n        my_tensor1 = torch.tensor(charToNum1, device=device)\n        data[\"val_charToNum\"][k] = my_tensor1\n        k += 1\n\n    data[\"source_len\"] = len(data[\"all_characters\"])\n    data[\"target_len\"] = len(data[\"all_characters_2\"])\n    return data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T01:53:28.636819Z","iopub.execute_input":"2025-05-12T01:53:28.637036Z","iopub.status.idle":"2025-05-12T01:53:28.645192Z","shell.execute_reply.started":"2025-05-12T01:53:28.637018Z","shell.execute_reply":"2025-05-12T01:53:28.644435Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"data = pre_processing(copy.copy(train_input), copy.copy(train_output))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T01:53:28.645814Z","iopub.execute_input":"2025-05-12T01:53:28.646040Z","iopub.status.idle":"2025-05-12T01:53:34.692154Z","shell.execute_reply.started":"2025-05-12T01:53:28.646024Z","shell.execute_reply":"2025-05-12T01:53:34.691614Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def pre_processing_validation(val_input, val_output):\n    data2 = {\n        \"all_characters\": [],\n        \"char_num_map\": {},\n        \"num_char_map\": {},\n        \"source_charToNum\": torch.zeros(len(val_input), 30, dtype=torch.int, device=device),\n        \"source_data\": val_input,\n        \"all_characters_2\": [],\n        \"char_num_map_2\": {},\n        \"num_char_map_2\": {},\n        \"val_charToNum\": torch.zeros(len(val_output), 23, dtype=torch.int, device=device),\n        \"target_data\": val_output,\n        \"source_len\": 0,\n        \"target_len\": 0\n    }\n    k = 0\n    m1 = data[\"char_num_map\"]\n    m2 = data[\"char_num_map_2\"]\n    for i in range(len(val_input)):\n        val_input[i] = \"{\" + val_input[i] + \"}\" * (29 - len(val_input[i]))\n        charToNum = []\n        for char in val_input[i]:\n            if char not in data2[\"all_characters\"]:\n                data2[\"all_characters\"].append(char)\n                index = m1[char]\n                data2[\"char_num_map\"][char] = index\n                data2[\"num_char_map\"][index] = char\n            else:\n                index = m1[char]\n            charToNum.append(index)\n        my_tensor = torch.tensor(charToNum, device=device)\n        data2[\"source_charToNum\"][k] = my_tensor\n\n        charToNum1 = []\n        val_output[i] = \"{\" + val_output[i] + \"}\" * (22 - len(val_output[i]))\n        for char in val_output[i]:\n            if char not in data2[\"all_characters_2\"]:\n                data2[\"all_characters_2\"].append(char)\n                index = m2[char]\n                data2[\"char_num_map_2\"][char] = index\n                data2[\"num_char_map_2\"][index] = char\n            else:\n                index = m2[char]\n            charToNum1.append(index)\n        my_tensor1 = torch.tensor(charToNum1, device=device)\n        data2[\"val_charToNum\"][k] = my_tensor1\n        k += 1\n\n    data2[\"source_len\"] = len(data2[\"all_characters\"])\n    data2[\"target_len\"] = len(data2[\"all_characters_2\"])\n    return data2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T01:53:34.692841Z","iopub.execute_input":"2025-05-12T01:53:34.693020Z","iopub.status.idle":"2025-05-12T01:53:34.701212Z","shell.execute_reply.started":"2025-05-12T01:53:34.693005Z","shell.execute_reply":"2025-05-12T01:53:34.700615Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"data2 = pre_processing_validation(copy.copy(val_input), copy.copy(val_output))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T01:53:34.701868Z","iopub.execute_input":"2025-05-12T01:53:34.702057Z","iopub.status.idle":"2025-05-12T01:53:35.242227Z","shell.execute_reply.started":"2025-05-12T01:53:34.702042Z","shell.execute_reply":"2025-05-12T01:53:35.241652Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class MyDataset(Dataset):\n    def __init__(self, x, y):\n        self.source = x\n        self.target = y\n\n    def __len__(self):\n        return len(self.source)\n\n    def __getitem__(self, idx):\n        source_data = self.source[idx]\n        target_data = self.target[idx]\n        return source_data, target_data\n\ndef validationAccuracy(encoder, decoder, batchsize, tf_ratio, cellType, bidirection):\n    dataLoader = dataLoaderFun(\"validation\", batchsize)\n    encoder.eval()\n    decoder.eval()\n    total_sequences = 0\n    total_correct_sequences = 0\n    total_char_matches = 0\n    total_characters = 0\n    total_loss = 0\n\n    lossFunction = nn.NLLLoss()\n\n    for source_batch, target_batch in dataLoader:\n        actual_batch_size = source_batch.shape[0]\n        total_sequences += actual_batch_size\n        total_characters += target_batch.numel()\n\n        encoder_initial_state = encoder.getInitialState(actual_batch_size)\n        if bidirection == \"Yes\":\n            reversed_batch = torch.flip(source_batch, dims=[1])\n            source_batch = (source_batch + reversed_batch) // 2\n        if cellType == 'LSTM':\n            encoder_initial_state = (encoder_initial_state, encoder.getInitialState(actual_batch_size))\n\n        encoder_states, _ = encoder(source_batch, encoder_initial_state)\n        decoder_current_state = encoder_states[-1, :, :, :]\n        encoder_final_layer_states = encoder_states[:, -1, :, :]\n        output_seq_len = target_batch.shape[1]\n\n        loss = 0\n        decoder_actual_output = []\n        randNumber = random.random()\n\n        for i in range(output_seq_len):\n            if i == 0:\n                decoder_current_input = torch.full((actual_batch_size, 1), 0, device=device)\n            else:\n                if randNumber < tf_ratio:\n                    decoder_current_input = target_batch[:, i].reshape(actual_batch_size, 1)\n                else:\n                    decoder_current_input = decoder_current_input.reshape(actual_batch_size, 1)\n            decoder_output, decoder_current_state, _ = decoder(decoder_current_input, decoder_current_state, encoder_final_layer_states)\n            topv, topi = decoder_output.topk(1)\n            decoder_current_input = topi.squeeze().detach()\n            decoder_actual_output.append(decoder_current_input)\n\n            decoder_output = decoder_output[:, -1, :]\n            curr_target_chars = target_batch[:, i].long()\n            loss += lossFunction(decoder_output, curr_target_chars)\n\n        total_loss += loss.item() / output_seq_len\n        decoder_actual_output = torch.cat(decoder_actual_output, dim=0).reshape(output_seq_len, actual_batch_size).transpose(0, 1)\n        total_correct_sequences += (decoder_actual_output == target_batch).all(dim=1).sum().item()\n        total_char_matches += (decoder_actual_output == target_batch).sum().item()\n\n    encoder.train()\n    decoder.train()\n\n    wandb.log({\n        'validation_loss': total_loss / len(dataLoader),\n        'validation_accuracy': total_correct_sequences / total_sequences,\n        'validation_char_accuracy': total_char_matches / total_characters\n    })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T01:53:35.244307Z","iopub.execute_input":"2025-05-12T01:53:35.244499Z","iopub.status.idle":"2025-05-12T01:53:35.254051Z","shell.execute_reply.started":"2025-05-12T01:53:35.244485Z","shell.execute_reply":"2025-05-12T01:53:35.253419Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self, hidden_size):\n        super(Attention, self).__init__()\n        self.Wa = nn.Linear(hidden_size, hidden_size)\n        self.Ua = nn.Linear(hidden_size, hidden_size)\n        self.Va = nn.Linear(hidden_size, 1)\n\n    def forward(self, query, keys):\n        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n        scores = scores.squeeze().unsqueeze(1)\n        weights = F.softmax(scores, dim=0)\n        weights = weights.permute(2, 1, 0)\n        keys = keys.permute(1, 0, 2)\n        context = torch.bmm(weights, keys)\n        return context, weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T01:53:35.254674Z","iopub.execute_input":"2025-05-12T01:53:35.254831Z","iopub.status.idle":"2025-05-12T01:53:35.271862Z","shell.execute_reply.started":"2025-05-12T01:53:35.254818Z","shell.execute_reply":"2025-05-12T01:53:35.271079Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, inputDim, embSize, encoderLayers, hiddenLayerNuerons, cellType, batch_size):\n        super(Encoder, self).__init__()\n        self.embedding = nn.Embedding(inputDim, embSize)\n        self.encoderLayers = encoderLayers\n        self.hiddenLayerNuerons = hiddenLayerNuerons\n        self.cellType = cellType\n        if cellType == 'GRU':\n            self.rnn = nn.GRU(embSize, hiddenLayerNuerons, num_layers=encoderLayers, batch_first=True)\n        elif cellType == 'RNN':\n            self.rnn = nn.RNN(embSize, hiddenLayerNuerons, num_layers=encoderLayers, batch_first=True)\n        else:\n            self.rnn = nn.LSTM(embSize, hiddenLayerNuerons, num_layers=encoderLayers, batch_first=True)\n\n    def forward(self, sourceBatch, encoderCurrState):\n        sequenceLength = sourceBatch.shape[1]\n        batch_size = sourceBatch.shape[0]\n        encoderStates = torch.zeros(sequenceLength, self.encoderLayers, batch_size, self.hiddenLayerNuerons, device=device)\n        for i in range(sequenceLength):\n            currInput = sourceBatch[:, i].reshape(batch_size, 1)\n            _, encoderCurrState = self.statesCalculation(currInput, encoderCurrState)\n            if self.cellType == 'LSTM':\n                encoderStates[i] = encoderCurrState[1]\n            else:\n                encoderStates[i] = encoderCurrState\n        return encoderStates, encoderCurrState\n\n    def statesCalculation(self, currentInput, prevState):\n        embdInput = self.embedding(currentInput)\n        output, prev_state = self.rnn(embdInput, prevState)\n        return output, prev_state\n\n    def getInitialState(self, batch_size):\n        return torch.zeros(self.encoderLayers, batch_size, self.hiddenLayerNuerons, device=device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T01:53:35.272703Z","iopub.execute_input":"2025-05-12T01:53:35.273432Z","iopub.status.idle":"2025-05-12T01:53:35.285293Z","shell.execute_reply.started":"2025-05-12T01:53:35.273407Z","shell.execute_reply":"2025-05-12T01:53:35.284790Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, outputDim, embSize, hiddenLayerNuerons, decoderLayers, cellType, dropout_p):\n        super(Decoder, self).__init__()\n        self.embedding = nn.Embedding(outputDim, embSize)\n        self.cellType = cellType\n        if cellType == 'GRU':\n            self.rnn = nn.GRU(embSize + hiddenLayerNuerons, hiddenLayerNuerons, num_layers=decoderLayers, batch_first=True)\n        elif cellType == 'RNN':\n            self.rnn = nn.RNN(embSize + hiddenLayerNuerons, hiddenLayerNuerons, num_layers=decoderLayers, batch_first=True)\n        else:\n            self.rnn = nn.LSTM(embSize + hiddenLayerNuerons, hiddenLayerNuerons, num_layers=decoderLayers, batch_first=True)\n        self.fc = nn.Linear(hiddenLayerNuerons, outputDim)\n        self.softmax = nn.LogSoftmax(dim=2)\n        self.dropout = nn.Dropout(dropout_p)\n        self.attention = Attention(hiddenLayerNuerons).to(device)\n\n    def forward(self, current_input, prev_state, encoder_final_layers):\n        if self.cellType == 'LSTM':\n            context, attn_weights = self.attention(prev_state[1][-1, :, :], encoder_final_layers)\n        else:\n            context, attn_weights = self.attention(prev_state[-1, :, :], encoder_final_layers)\n        embd_input = self.embedding(current_input)\n        curr_embd = F.relu(embd_input)\n        input_gru = torch.cat((curr_embd, context), dim=2)\n        output, prev_state = self.rnn(input_gru, prev_state)\n        output = self.dropout(output)\n        output = self.softmax(self.fc(output))\n        return output, prev_state, attn_weights\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T01:53:35.285931Z","iopub.execute_input":"2025-05-12T01:53:35.286165Z","iopub.status.idle":"2025-05-12T01:53:35.303755Z","shell.execute_reply.started":"2025-05-12T01:53:35.286145Z","shell.execute_reply":"2025-05-12T01:53:35.303220Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def dataLoaderFun(dataName, batch_size):\n    if dataName == 'train':\n        dataset = MyDataset(data[\"source_charToNum\"], data['val_charToNum'])\n        return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n    else:\n        dataset = MyDataset(data2[\"source_charToNum\"], data2['val_charToNum'])\n        return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\ndef train(embSize, encoderLayers, decoderLayers, hiddenLayerNuerons, cellType, bidirection, dropout, epochs, batchsize, learningRate, optimizer, tf_ratio):\n    dataLoader = dataLoaderFun(\"train\", batchsize)\n    encoder = Encoder(data[\"source_len\"], embSize, encoderLayers, hiddenLayerNuerons, cellType, batchsize).to(device)\n    decoder = Decoder(data[\"target_len\"], embSize, hiddenLayerNuerons, encoderLayers, cellType, dropout).to(device)\n    if optimizer == 'Adam':\n        encoderOptimizer = optim.Adam(encoder.parameters(), lr=learningRate)\n        decoderOptimizer = optim.Adam(decoder.parameters(), lr=learningRate)\n    else:\n        encoderOptimizer = optim.NAdam(encoder.parameters(), lr=learningRate)\n        decoderOptimizer = optim.NAdam(decoder.parameters(), lr=learningRate)\n    lossFunction = nn.NLLLoss()\n    for epoch in range(epochs):\n        train_accuracy = 0\n        train_loss = 0\n        for batch_num, (source_batch, target_batch) in enumerate(dataLoader):\n            actual_batch_size = source_batch.shape[0]\n            encoder_initial_state = encoder.getInitialState(actual_batch_size)\n            if bidirection == \"Yes\":\n                reversed_batch = torch.flip(source_batch, dims=[1])\n                source_batch = (source_batch + reversed_batch) // 2\n            if cellType == 'LSTM':\n                encoder_initial_state = (encoder_initial_state, encoder.getInitialState(actual_batch_size))\n            encoder_states, dummy = encoder(source_batch, encoder_initial_state)\n            decoder_current_state = dummy\n            encoder_final_layer_states = encoder_states[:, -1, :, :]\n            loss = 0\n            output_seq_len = target_batch.shape[1]\n            decoder_actual_output = []\n            randNumber = random.random()\n            for i in range(output_seq_len):\n                if i == 0:\n                    decoder_current_input = torch.full((actual_batch_size, 1), 0, device=device)\n                else:\n                    if randNumber < tf_ratio:\n                        decoder_current_input = target_batch[:, i].reshape(actual_batch_size, 1)\n                    else:\n                        decoder_current_input = decoder_current_input.reshape(actual_batch_size, 1)\n                decoder_output, decoder_current_state, _ = decoder(decoder_current_input, decoder_current_state, encoder_final_layer_states)\n                topv, topi = decoder_output.topk(1)\n                decoder_current_input = topi.squeeze().detach()\n                decoder_actual_output.append(decoder_current_input)\n                decoder_output = decoder_output[:, -1, :]\n                curr_target_chars = target_batch[:, i].type(dtype=torch.long)\n                loss += (lossFunction(decoder_output, curr_target_chars))\n            decoder_actual_output = torch.cat(decoder_actual_output, dim=0).reshape(output_seq_len, actual_batch_size).transpose(0, 1)\n            train_accuracy += (decoder_actual_output == target_batch).all(dim=1).sum().item()\n            train_loss += (loss.item() / output_seq_len)\n            encoderOptimizer.zero_grad()\n            decoderOptimizer.zero_grad()\n            loss.backward()\n            encoderOptimizer.step()\n            decoderOptimizer.step()\n\n        # ✅ Logging train metrics here\n        wandb.log({'train_accuracy': train_accuracy / len(data[\"source_charToNum\"])})\n        wandb.log({'train_loss': train_loss / len(dataLoader)})\n\n        validationAccuracy(encoder, decoder, batchsize, tf_ratio, cellType, bidirection)\n\n\ndef numToCharConverter(inputArray, outputArray, data):\n    mp = data['num_char_map_2']\n    for row1, row2 in zip(inputArray, outputArray):\n        t1 = ''.join([mp[e1.item()] for e1 in row1])\n        t2 = ''.join([mp[e2.item()] for e2 in row2])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T01:53:35.304515Z","iopub.execute_input":"2025-05-12T01:53:35.304739Z","iopub.status.idle":"2025-05-12T01:53:35.320321Z","shell.execute_reply.started":"2025-05-12T01:53:35.304715Z","shell.execute_reply":"2025-05-12T01:53:35.319655Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def main_fun():\n    wandb.init(project='DA6401_Assignment_3')\n    params = wandb.config\n    with wandb.init(project='DA6401_Assignment_3', name='embedding'+str(params.embSize)+'cellType'+params.cellType+'batchSize'+str(params.batchsize)) as run:\n        train(params.embSize, params.encoderLayers, params.decoderLayers, params.hiddenLayerNuerons, params.cellType, params.bidirection, params.dropout, params.epochs, params.batchsize, params.learningRate, params.optimizer, params.tf_ratio)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T01:53:35.321064Z","iopub.execute_input":"2025-05-12T01:53:35.321239Z","iopub.status.idle":"2025-05-12T01:53:35.335727Z","shell.execute_reply.started":"2025-05-12T01:53:35.321224Z","shell.execute_reply":"2025-05-12T01:53:35.335172Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"sweep_params = {\n    'method': 'bayes',\n    'name': 'DA6401_Assignment_3',\n    'metric': {\n        'goal': 'maximize',\n        'name': 'validation_accuracy',\n    },\n    'parameters': {\n        'embSize': {'values': [16, 32, 64]},\n        'encoderLayers': {'values': [1, 5, 10]},\n        'decoderLayers': {'values': [1, 5, 10]},\n        'hiddenLayerNuerons': {'values': [64, 256, 512]},\n        'cellType': {'values': ['GRU', 'RNN']},\n        'bidirection': {'values': ['no', 'Yes']},\n        'dropout': {'values': [0, 0.2, 0.3]},\n        'epochs': {'values': [10, 15]},\n        'batchsize': {'values': [32, 64]},\n        'learningRate': {'values': [1e-2, 1e-3, 1e-4]},\n        'optimizer': {'values': ['Adam', 'Nadam']},\n        'tf_ratio': {'values': [0.2, 0.4, 0.5]}\n    }\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T01:53:35.336430Z","iopub.execute_input":"2025-05-12T01:53:35.336680Z","iopub.status.idle":"2025-05-12T01:53:35.351584Z","shell.execute_reply.started":"2025-05-12T01:53:35.336660Z","shell.execute_reply":"2025-05-12T01:53:35.350879Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"sweepId = wandb.sweep(sweep_params, project='DA6401_Assignment_3')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T01:53:35.352451Z","iopub.execute_input":"2025-05-12T01:53:35.352651Z","iopub.status.idle":"2025-05-12T01:53:35.668904Z","shell.execute_reply.started":"2025-05-12T01:53:35.352629Z","shell.execute_reply":"2025-05-12T01:53:35.668299Z"}},"outputs":[{"name":"stdout","text":"Create sweep with ID: kaqf36xx\nSweep URL: https://wandb.ai/cs23m059-iit-madras/DA6401_Assignment_3/sweeps/kaqf36xx\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"wandb.agent(\"4i8njfsm\", function=main_fun, count=30, entity=\"cs23m059-iit-madras\", project=\"DA6401_Assignment_3\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T01:53:35.669619Z","iopub.execute_input":"2025-05-12T01:53:35.669827Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: rwm6tos9 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchsize: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirection: no\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcellType: RNN\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoderLayers: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembSize: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tencoderLayers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thiddenLayerNuerons: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearningRate: 0.0001\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Nadam\n\u001b[34m\u001b[1mwandb\u001b[0m: \ttf_ratio: 0.5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'DA6401_Assignment_3' when running a sweep."},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250512_015342-rwm6tos9</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cs23m059-iit-madras/DA6401_Assignment_3/runs/rwm6tos9' target=\"_blank\">laced-sweep-36</a></strong> to <a href='https://wandb.ai/cs23m059-iit-madras/DA6401_Assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23m059-iit-madras/DA6401_Assignment_3/sweeps/4i8njfsm' target=\"_blank\">https://wandb.ai/cs23m059-iit-madras/DA6401_Assignment_3/sweeps/4i8njfsm</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cs23m059-iit-madras/DA6401_Assignment_3' target=\"_blank\">https://wandb.ai/cs23m059-iit-madras/DA6401_Assignment_3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/cs23m059-iit-madras/DA6401_Assignment_3/sweeps/4i8njfsm' target=\"_blank\">https://wandb.ai/cs23m059-iit-madras/DA6401_Assignment_3/sweeps/4i8njfsm</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cs23m059-iit-madras/DA6401_Assignment_3/runs/rwm6tos9' target=\"_blank\">https://wandb.ai/cs23m059-iit-madras/DA6401_Assignment_3/runs/rwm6tos9</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'DA6401_Assignment_3' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>▁▄▅▆▆▇▇▇███████</td></tr><tr><td>train_loss</td><td>█▄▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▃▅▅▆█▆▇▇▇▆▇▇▇█</td></tr><tr><td>validation_char_accuracy</td><td>▁▅▆▇▇█▇▇██▇█▇██</td></tr><tr><td>validation_loss</td><td>█▄▂▂▂▁▂▂▁▁▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>0.69637</td></tr><tr><td>train_loss</td><td>0.10414</td></tr><tr><td>validation_accuracy</td><td>0.71946</td></tr><tr><td>validation_char_accuracy</td><td>0.97048</td></tr><tr><td>validation_loss</td><td>0.10408</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">laced-sweep-36</strong> at: <a href='https://wandb.ai/cs23m059-iit-madras/DA6401_Assignment_3/runs/rwm6tos9' target=\"_blank\">https://wandb.ai/cs23m059-iit-madras/DA6401_Assignment_3/runs/rwm6tos9</a><br> View project at: <a href='https://wandb.ai/cs23m059-iit-madras/DA6401_Assignment_3' target=\"_blank\">https://wandb.ai/cs23m059-iit-madras/DA6401_Assignment_3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250512_015342-rwm6tos9/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: aytssyjf with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchsize: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirection: no\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcellType: RNN\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoderLayers: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembSize: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tencoderLayers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n\u001b[34m\u001b[1mwandb\u001b[0m: \thiddenLayerNuerons: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearningRate: 0.0001\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Nadam\n\u001b[34m\u001b[1mwandb\u001b[0m: \ttf_ratio: 0.5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'DA6401_Assignment_3' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250512_021126-aytssyjf</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cs23m059-iit-madras/DA6401_Assignment_3/runs/aytssyjf' target=\"_blank\">young-sweep-37</a></strong> to <a href='https://wandb.ai/cs23m059-iit-madras/DA6401_Assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23m059-iit-madras/DA6401_Assignment_3/sweeps/4i8njfsm' target=\"_blank\">https://wandb.ai/cs23m059-iit-madras/DA6401_Assignment_3/sweeps/4i8njfsm</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cs23m059-iit-madras/DA6401_Assignment_3' target=\"_blank\">https://wandb.ai/cs23m059-iit-madras/DA6401_Assignment_3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/cs23m059-iit-madras/DA6401_Assignment_3/sweeps/4i8njfsm' target=\"_blank\">https://wandb.ai/cs23m059-iit-madras/DA6401_Assignment_3/sweeps/4i8njfsm</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cs23m059-iit-madras/DA6401_Assignment_3/runs/aytssyjf' target=\"_blank\">https://wandb.ai/cs23m059-iit-madras/DA6401_Assignment_3/runs/aytssyjf</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'DA6401_Assignment_3' when running a sweep."},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}