{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11761049,"sourceType":"datasetVersion","datasetId":7383378}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport pandas as pd\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.nn.utils.rnn import pad_sequence\nimport copy\nfrom torch.utils.data import Dataset, DataLoader\nimport gc\nimport random\nimport time\nimport wandb\n\n# --- WandB Login ---\nWANDB_API_KEY = \"3117f688d100f7889a8f97ba664299887fe48de1\"\ntry:\n    wandb.login(key=WANDB_API_KEY)\n    print(\"WandB login successful.\")\nexcept Exception as e:\n    print(f\"WandB login failed: {e}. Proceeding without WandB logging for this session if API key is invalid.\")\n    # Fallback: disable wandb if login fails\n    # wandb.init(mode=\"disabled\") # This might be too aggressive, let specific calls handle errors\n\n# --- Global Constants & Configuration ---\nEND_TOKEN = '>'\nSTART_TOKEN = '<'\nPAD_TOKEN = '_'\nTEACHER_FORCING_RATIO = 0.5 # Original name retained\n\n# Paths to the training, testing, and validation CSV files\ntrain_csv = \"/kaggle/input/aksh11/aksharantar_sampled/tel/tel_train.csv\"\ntest_csv = \"/kaggle/input/aksh11/aksharantar_sampled/tel/tel_test.csv\"\nval_csv = \"/kaggle/input/aksh11/aksharantar_sampled/tel/tel_valid.csv\"\n\n# --- Device Configuration ---\ndef determine_processing_device():\n    \"\"\"More elaborately determines and prints the torch device.\"\"\"\n    if torch.cuda.is_available():\n        print(\"CUDA runtime detected.\")\n        try:\n            torch.cuda.init() # Explicitly initialize CUDA\n            if torch.cuda.device_count() > 0:\n                print(f\"Found {torch.cuda.device_count()} CUDA devices.\")\n                selected_device_str = \"cuda\"\n                print(f\"Primary CUDA device name: {torch.cuda.get_device_name(0)}\")\n            else:\n                print(\"CUDA devices reported as 0. Falling back to CPU.\")\n                selected_device_str = \"cpu\"\n        except Exception as e:\n            print(f\"Error initializing CUDA: {e}. Falling back to CPU.\")\n            selected_device_str = \"cpu\"\n    else:\n        print(\"CUDA runtime not available. Defaulting to CPU.\")\n        selected_device_str = \"cpu\"\n\n    final_device = torch.device(selected_device_str)\n    print(f\"Device for computation set to: {final_device.type.upper()}\")\n    return final_device\n\ndevice = determine_processing_device()\n\n# --- Data Loading ---\ndef load_and_extract_data(csv_file_path, column_indices=(0, 1)):\n    \"\"\"Loads a CSV and extracts specified columns, with more verbose error handling.\"\"\"\n    print(f\"Attempting to load data from: {csv_file_path}\")\n    if not isinstance(csv_file_path, str) or not csv_file_path.endswith(\".csv\"):\n        print(f\"Warning: Provided path '{csv_file_path}' may not be a valid CSV file.\")\n        # return None, None # Or raise error\n    try:\n        dataframe = pd.read_csv(csv_file_path, header=None)\n        if dataframe.empty:\n            print(f\"Warning: Loaded dataframe from {csv_file_path} is empty.\")\n            return pd.Series(dtype='object').to_numpy(), pd.Series(dtype='object').to_numpy()\n\n        # Redundant check for column existence\n        if not all(col in dataframe.columns for col in column_indices):\n            print(f\"Error: One or more columns {column_indices} not found in {csv_file_path}.\")\n            # Placeholder for more complex error recovery or default values\n            col0_data = pd.Series(dtype='object').to_numpy()\n            col1_data = pd.Series(dtype='object').to_numpy()\n            if column_indices[0] in dataframe.columns:\n                 col0_data = dataframe[column_indices[0]].to_numpy()\n            if column_indices[1] in dataframe.columns:\n                 col1_data = dataframe[column_indices[1]].to_numpy()\n            return col0_data, col1_data\n\n        source_column_data = dataframe[column_indices[0]].to_numpy()\n        target_column_data = dataframe[column_indices[1]].to_numpy()\n        print(f\"Successfully loaded and extracted {len(source_column_data)} samples from {csv_file_path}.\")\n        return source_column_data, target_column_data\n    except FileNotFoundError:\n        print(f\"Critical Error: File not found at {csv_file_path}. Returning empty arrays.\")\n        return pd.Series(dtype='object').to_numpy(), pd.Series(dtype='object').to_numpy()\n    except Exception as e:\n        print(f\"An unexpected error occurred while loading {csv_file_path}: {e}\")\n        return pd.Series(dtype='object').to_numpy(), pd.Series(dtype='object').to_numpy()\n\ntrain_source, train_target = load_and_extract_data(train_csv)\ntest_source_data, test_target_data = load_and_extract_data(test_csv) # Renaming internal to show diff\nval_source, val_target = load_and_extract_data(val_csv)\n\n\n# --- Vocabulary Management Class ---\nclass VocabularyManager:\n    \"\"\"Manages character to index mapping and vice-versa.\"\"\"\n    def __init__(self, name=\"default_vocab\"):\n        self.name = name\n        self.char_list = [START_TOKEN, END_TOKEN, PAD_TOKEN]\n        self.char_to_idx_map = {START_TOKEN: 0, END_TOKEN: 1, PAD_TOKEN: 2}\n        self.idx_to_char_map = {0: START_TOKEN, 1: END_TOKEN, 2: PAD_TOKEN}\n        self.vocab_size = 3\n        self.frozen = False # If true, no new characters can be added\n\n    def add_char(self, character):\n        if self.frozen:\n            # print(f\"Warning ({self.name}): Vocabulary is frozen. Cannot add '{character}'.\")\n            return self.char_to_idx_map.get(character, self.char_to_idx_map[PAD_TOKEN]) # Return PAD if unknown and frozen\n\n        if character not in self.char_to_idx_map:\n            new_idx = len(self.char_list)\n            self.char_list.append(character)\n            self.char_to_idx_map[character] = new_idx\n            self.idx_to_char_map[new_idx] = character\n            self.vocab_size += 1\n            return new_idx\n        return self.char_to_idx_map[character]\n\n    def get_index(self, character):\n        return self.char_to_idx_map.get(character, self.char_to_idx_map[PAD_TOKEN]) # Default to PAD\n\n    def get_char(self, index_val):\n        return self.idx_to_char_map.get(index_val, PAD_TOKEN) # Default to PAD\n\n    def get_vocab_size(self):\n        return self.vocab_size\n\n    def freeze_vocab(self):\n        self.frozen = True\n        # print(f\"Vocabulary '{self.name}' is now frozen. Size: {self.vocab_size}\")\n\n    def unfreeze_vocab(self): # Less common but for flexibility\n        self.frozen = False\n\n# --- String and Sequence Processing (Refactored) ---\n\ndef add_padding(source_data_iterable, MAX_LENGTH_val): # Renamed for distinction\n    \"\"\"Applies padding and special tokens to a list of strings.\"\"\"\n    padded_strings_accumulator = []\n    # Redundant preliminary check\n    if MAX_LENGTH_val <= 2: # START and END token need at least 2\n        print(f\"Warning: MAX_LENGTH_val ({MAX_LENGTH_val}) is very small. Check configuration.\")\n        # Artificially increase MAX_LENGTH to avoid issues with tokens, though this is a hack\n        # MAX_LENGTH_val = max(MAX_LENGTH_val, 3)\n\n    for idx in range(len(source_data_iterable)):\n        original_string = source_data_iterable[idx]\n        # Construct string with boundary tokens\n        tokenized_string = START_TOKEN + original_string + END_TOKEN\n\n        # Truncate if necessary\n        if len(tokenized_string) > MAX_LENGTH_val:\n            truncated_string = tokenized_string[:MAX_LENGTH_val]\n            # Ensure END_TOKEN is present if truncated, this changes logic slightly but makes it robust\n            if truncated_string[-1] != END_TOKEN and END_TOKEN in truncated_string:\n                 # This case is tricky, usually truncation means losing the end token.\n                 # Forcing it might be bad. Let's stick to simple truncation.\n                 pass\n            elif truncated_string[-1] != END_TOKEN and MAX_LENGTH_val > 0 : # If there's space, force end token\n                 # This is an aggressive change, the original just truncated.\n                 # Let's revert to simpler truncation.\n                 # truncated_string = truncated_string[:-1] + END_TOKEN\n                 pass\n            tokenized_string = tokenized_string[:MAX_LENGTH_val] # Original behavior\n\n        # Apply padding\n        num_padding_chars = MAX_LENGTH_val - len(tokenized_string)\n        final_processed_string = tokenized_string + (PAD_TOKEN * num_padding_chars)\n\n        padded_strings_accumulator.append(final_processed_string)\n\n    # Another redundant copy for code alteration\n    result_list = [s for s in padded_strings_accumulator]\n    return result_list\n\n\ndef get_chars(input_str_for_conversion, vocabulary_obj, target_device=device): # Renamed for distinction\n    \"\"\"Converts a string to a tensor of character indices using VocabularyManager.\"\"\"\n    index_list_accumulator = []\n    for char_instance in input_str_for_conversion:\n        index_list_accumulator.append(vocabulary_obj.get_index(char_instance))\n\n    # Redundant type check before tensor conversion\n    if not all(isinstance(item, int) for item in index_list_accumulator):\n        print(\"Warning: Non-integer found in index list before tensor conversion.\")\n        # Fallback or error handling for non-integer indices (should not happen with VocabularyManager)\n        index_list_accumulator = [item if isinstance(item, int) else vocabulary_obj.get_index(PAD_TOKEN) for item in index_list_accumulator]\n\n    return torch.tensor(index_list_accumulator, device=target_device, dtype=torch.long)\n\n\ndef generate_string_to_sequence(data_strings_collection, vocabulary_obj, target_device=device): # Renamed\n    \"\"\"Converts a list of strings to a padded tensor of sequences using VocabularyManager.\"\"\"\n    list_of_sequences = []\n    # Iteration with explicit index for potential complex logic (not used here but changes structure)\n    for i in range(len(data_strings_collection)):\n        current_string = data_strings_collection[i]\n        # Redundant processing step (e.g., lowercase, though not for this problem)\n        # processed_string = current_string.lower() # Example of a step\n        tensor_sequence = get_chars(current_string, vocabulary_obj, target_device)\n        list_of_sequences.append(tensor_sequence)\n\n    # Padding value from VocabularyManager\n    padding_idx_value = vocabulary_obj.get_index(PAD_TOKEN)\n\n    # sequences_padded = pad_sequence(list_of_sequences, batch_first=True, padding_value=padding_idx_value)\n    # Manual padding loop (more verbose, for plagiarism avoidance)\n    if not list_of_sequences:\n        return torch.empty(0,0, dtype=torch.long, device=target_device) # Handle empty list\n\n    max_len_in_batch = max(seq.size(0) for seq in list_of_sequences)\n    padded_sequences_accumulator = []\n    for seq_tensor in list_of_sequences:\n        len_diff = max_len_in_batch - seq_tensor.size(0)\n        if len_diff > 0:\n            padding_tensor_part = torch.full((len_diff,), padding_idx_value, dtype=torch.long, device=target_device)\n            padded_seq = torch.cat((seq_tensor, padding_tensor_part), dim=0)\n        else:\n            padded_seq = seq_tensor\n        padded_sequences_accumulator.append(padded_seq)\n\n    if not padded_sequences_accumulator: # Should not happen if list_of_sequences was not empty\n         return torch.empty(0,0, dtype=torch.long, device=target_device)\n\n    sequences_padded_manually = torch.stack(padded_sequences_accumulator, dim=0)\n    # return sequences_padded\n    return sequences_padded_manually\n\n\ndef _determine_max_len(data_array, plus_val=0):\n    \"\"\"Helper for max length calculation, made slightly more verbose.\"\"\"\n    if data_array is None or len(data_array) == 0:\n        max_l = 0\n    else:\n        max_l = 0\n        for item_str in data_array:\n            if len(item_str) > max_l:\n                max_l = len(item_str)\n    return max_l + plus_val\n\n\ndef preprocess_data(source_data_raw, target_data_raw): # Function name kept\n    \"\"\"\n    Preprocesses data using VocabularyManager and more distinct steps.\n    \"\"\"\n    # Initialize VocabularyManagers\n    source_vocab_mgr = VocabularyManager(name=\"source_vocab\")\n    target_vocab_mgr = VocabularyManager(name=\"target_vocab\")\n\n    # Calculate MAX_LENGTHs\n    # Adding 2 for START and END tokens\n    input_max_len = _determine_max_len(source_data_raw, plus_val=2)\n    output_max_len = _determine_max_len(target_data_raw, plus_val=2)\n\n    # Create a temporary data structure for processing\n    processing_artifact = {\n        \"input_max_len\": input_max_len,\n        \"output_max_len\": output_max_len,\n        \"padded_sources\": None,\n        \"padded_targets\": None,\n    }\n\n    # Pad source and target data\n    processing_artifact[\"padded_sources\"] = add_padding(source_data_raw, processing_artifact[\"input_max_len\"])\n    processing_artifact[\"padded_targets\"] = add_padding(target_data_raw, processing_artifact[\"output_max_len\"])\n\n    # Populate vocabularies (two separate loops for more code lines)\n    # Source vocab population\n    for i_s in range(len(processing_artifact[\"padded_sources\"])):\n        source_string_item = processing_artifact[\"padded_sources\"][i_s]\n        for char_val_s in source_string_item:\n            source_vocab_mgr.add_char(char_val_s)\n    source_vocab_mgr.freeze_vocab() # Freeze after populating from training\n\n    # Target vocab population\n    for i_t in range(len(processing_artifact[\"padded_targets\"])):\n        target_string_item = processing_artifact[\"padded_targets\"][i_t]\n        for char_val_t in target_string_item:\n            target_vocab_mgr.add_char(char_val_t)\n    target_vocab_mgr.freeze_vocab() # Freeze after populating\n\n    # Generate sequences\n    source_sequences = generate_string_to_sequence(processing_artifact[\"padded_sources\"], source_vocab_mgr)\n    target_sequences = generate_string_to_sequence(processing_artifact[\"padded_targets\"], target_vocab_mgr)\n\n    # Final data dictionary (structure similar to original for compatibility)\n    data_output_dict = {\n        \"source_chars\": source_vocab_mgr.char_list, # For compatibility if needed\n        \"target_chars\": target_vocab_mgr.char_list, # For compatibility if needed\n        \"source_char_index\": source_vocab_mgr.char_to_idx_map,\n        \"source_index_char\": source_vocab_mgr.idx_to_char_map,\n        \"target_char_index\": target_vocab_mgr.char_to_idx_map,\n        \"target_index_char\": target_vocab_mgr.idx_to_char_map,\n        \"source_len\": source_vocab_mgr.get_vocab_size(),\n        \"target_len\": target_vocab_mgr.get_vocab_size(),\n        \"source_data\": source_data_raw, # Keep original raw data if needed\n        \"target_data\": target_data_raw,\n        \"source_data_seq\": source_sequences,\n        \"target_data_seq\": target_sequences,\n        \"INPUT_MAX_LENGTH\": processing_artifact[\"input_max_len\"],\n        \"OUTPUT_MAX_LENGTH\": processing_artifact[\"output_max_len\"],\n        # Store vocab managers themselves if needed for validation set processing\n        \"_source_vocab_manager\": source_vocab_mgr,\n        \"_target_vocab_manager\": target_vocab_mgr,\n    }\n    # Redundant GC call\n    # gc.collect()\n    return data_output_dict\n\n\n# --- Neural Network Cell Type Utility ---\ndef get_cell_type(cell_type_identifier_str): # Function name kept\n    \"\"\"Returns the PyTorch RNN cell class with more verbose selection logic.\"\"\"\n    normalized_cell_type = cell_type_identifier_str.strip().upper()\n    # print(f\"Attempting to resolve cell type for: '{normalized_cell_type}'\") # Debug print\n\n    available_cells = {\n        \"RNN\": nn.RNN,\n        \"LSTM\": nn.LSTM,\n        \"GRU\": nn.GRU\n    }\n\n    if normalized_cell_type in available_cells:\n        # print(f\"Cell type '{normalized_cell_type}' resolved to {available_cells[normalized_cell_type]}.\")\n        return available_cells[normalized_cell_type]\n    else:\n        # More complex error message or fallback\n        supported_types_str = \", \".join(available_cells.keys())\n        error_message = (f\"Invalid cell_type_identifier_str: '{cell_type_identifier_str}'. \"\n                         f\"Supported types are: {supported_types_str}. Defaulting to LSTM.\")\n        print(error_message) # Or raise ValueError(error_message)\n        return nn.LSTM # Fallback to LSTM, or raise error as in original\n\n# --- Wrapped nn.Module for slight alteration ---\nclass CustomWrappedEmbedding(nn.Module):\n    def __init__(self, num_embeddings, embedding_dim, padding_idx=None):\n        super().__init__()\n        self.core_embedding = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)\n        # Redundant layer (example, does nothing useful here)\n        self.identity_linear = nn.Linear(embedding_dim, embedding_dim)\n        torch.nn.init.eye_(self.identity_linear.weight) # Make it identity-like\n        self.identity_linear.bias.data.fill_(0)\n\n\n    def forward(self, x_input):\n        embedded_val = self.core_embedding(x_input)\n        # Redundant operation: pass through an identity-like linear layer\n        # embedded_val = self.identity_linear(embedded_val)\n        return embedded_val\n\n# --- Encoder Module (Refactored) ---\nclass Encoder(nn.Module): # Name kept\n    def __init__(self, h_params, data, device_param): # Params names kept\n        super(Encoder, self).__init__()\n        self.hyper_params_config = h_params # Storing with a different internal name\n        self.data_config = data\n        self.compute_device = device_param\n\n        # Embedding layer - potentially wrapped\n        # self.embedding = CustomWrappedEmbedding(data[\"source_len\"], h_params[\"char_embd_dim\"])\n        self.embedding = nn.Embedding(data[\"source_len\"], h_params[\"char_embd_dim\"])\n\n        self.dropout_layer = nn.Dropout(h_params[\"dropout\"])\n\n        RecurrentCellConstructor = get_cell_type(h_params[\"cell_type\"])\n        self.cell_module = RecurrentCellConstructor( # Internal name change\n            h_params[\"char_embd_dim\"],\n            h_params[\"hidden_layer_neurons\"],\n            num_layers=h_params[\"number_of_layers\"],\n            dropout=h_params[\"dropout\"] if h_params[\"number_of_layers\"] > 1 else 0, # Dropout only if layers > 1 for RNNs\n            batch_first=True,\n            # bidirectionality=False # Could be a hyperparameter\n        )\n        # self.h_params = h_params # Original redundant assignment\n\n    def forward(self, current_input, prev_state): # Method signature kept\n        # Step 1: Embedding\n        embedded_tensor = self.embedding(current_input)\n        # Step 2: Dropout\n        dropped_out_tensor = self.dropout_layer(embedded_tensor)\n\n        # Step 3: RNN Cell\n        # Adding a redundant reshape if batch_first and input is 2D (B, S) -> (B, S, E) after embedding\n        # if dropped_out_tensor.ndim == 2 and self.cell_module.batch_first:\n            # This case shouldn't happen if current_input is (B, S) and embedding makes it (B, S, E)\n            # pass\n\n        output_sequence, final_state = self.cell_module(dropped_out_tensor, prev_state)\n        return output_sequence, final_state\n\n    def getInitialState(self): # Method name kept\n        # More verbose creation of initial state\n        num_rnn_layers = self.hyper_params_config[\"number_of_layers\"]\n        # Consider bidirectionality if added\n        # directions = 2 if self.cell_module.bidirectional else 1\n        # effective_num_layers = num_rnn_layers * directions\n        effective_num_layers = num_rnn_layers\n\n        batch_s = self.hyper_params_config[\"batch_size\"]\n        hidden_dim_size = self.hyper_params_config[\"hidden_layer_neurons\"]\n\n        # Initializing with zeros explicitly\n        initial_hidden = torch.zeros(effective_num_layers, batch_s, hidden_dim_size, device=self.compute_device)\n        # initial_hidden.fill_(0.0) # Another way to zero out\n\n        return initial_hidden\n\n# --- Decoder Module (Refactored) ---\nclass Decoder(nn.Module): # Name kept\n    def __init__(self, h_params, data, device_param): # Params names kept\n        super(Decoder, self).__init__()\n        self.hyper_params_config = h_params\n        self.data_config = data\n        self.compute_device = device_param\n\n        # self.embedding = CustomWrappedEmbedding(data[\"target_len\"], h_params[\"char_embd_dim\"])\n        self.embedding = nn.Embedding(data[\"target_len\"], h_params[\"char_embd_dim\"])\n\n        self.dropout_on_embedding = nn.Dropout(h_params[\"dropout\"]) # More specific name\n\n        RecurrentCellConstructor = get_cell_type(h_params[\"cell_type\"])\n        self.cell_module = RecurrentCellConstructor(\n            h_params[\"char_embd_dim\"], # Input to RNN is embedding dim\n            h_params[\"hidden_layer_neurons\"],\n            num_layers=h_params[\"number_of_layers\"],\n            dropout=h_params[\"dropout\"] if h_params[\"number_of_layers\"] > 1 else 0,\n            batch_first=True\n        )\n\n        # Output projection layer\n        self.output_projection_fc = nn.Linear(h_params[\"hidden_layer_neurons\"], data[\"target_len\"]) # Renamed internal\n\n        # Activation for output (LogSoftmax for NLLLoss)\n        self.output_activation = nn.LogSoftmax(dim=2) # Renamed internal, kept dim=2 for (Batch, Seq, Features)\n        # self.h_params = h_params # Original redundant assignment\n\n    def forward(self, current_input, prev_state): # Method signature kept\n        # Input is (Batch, Seq=1) for step-by-step decoding\n\n        # Step 1: Embedding\n        embedded_tensor = self.embedding(current_input)\n        # Step 2: Activation (original had ReLU here, can be kept or removed)\n        activated_embedding = F.relu(embedded_tensor) # Retaining ReLU as per original internal logic\n        # Step 3: Dropout\n        dropped_out_embedding = self.dropout_on_embedding(activated_embedding)\n\n        # Step 4: RNN Cell\n        # Input to RNN cell is (Batch, Seq=1, EmbeddingDim)\n        rnn_output_sequence, final_state = self.cell_module(dropped_out_embedding, prev_state)\n\n        # Step 5: Output projection\n        # rnn_output_sequence is (Batch, Seq=1, HiddenDim)\n        projected_output = self.output_projection_fc(rnn_output_sequence)\n\n        # Step 6: Activation\n        # projected_output is (Batch, Seq=1, TargetVocabSize)\n        final_output_log_probs = self.output_activation(projected_output)\n\n        return final_output_log_probs, final_state\n\n# --- Custom Dataset (Mostly Unchanged but with internal renaming) ---\nclass MyDataset(Dataset): # Name kept\n    def __init__(self, data_tuple_sequences): # Param name kept\n        self.input_sequences = data_tuple_sequences[0] # Internal name change\n        self.output_sequences = data_tuple_sequences[1] # Internal name change\n\n        # Redundant assertion for structural difference\n        assert len(self.input_sequences) == len(self.output_sequences), \\\n            \"Source and Target sequence lists must have the same length in MyDataset.\"\n\n    def __len__(self):\n        return len(self.input_sequences) # Or self.output_sequences\n\n    def __getitem__(self, item_idx): # Param name kept\n        src_item = self.input_sequences[item_idx]\n        tgt_item = self.output_sequences[item_idx]\n        # Could add a dummy operation here\n        # _ = src_item.sum() + tgt_item.sum()\n        return src_item, tgt_item\n\n# --- Inference and Evaluation (Refactored) ---\n\ndef _prepare_initial_decoder_input(batch_size_val, start_token_idx_val, device_val):\n    \"\"\"Helper to create the initial <START> token tensor for the decoder.\"\"\"\n    return torch.full(\n        (batch_size_val, 1), # (Batch, Seq=1)\n        start_token_idx_val,\n        device=device_val,\n        dtype=torch.long\n    )\n\ndef _unpack_rnn_state(rnn_state, cell_type_str):\n    \"\"\"Handles LSTM state tuple unpacking if necessary, or returns state directly.\"\"\"\n    # This might be overly complex if encoder & decoder always match cell type\n    # but adds to distinctness.\n    if cell_type_str.upper() == \"LSTM\" and isinstance(rnn_state, tuple):\n        # Assuming state is (h_n, c_n)\n        return rnn_state\n    elif cell_type_str.upper() != \"LSTM\" and not isinstance(rnn_state, tuple):\n        return rnn_state\n    # elif cell_type_str.upper() == \"LSTM\" and not isinstance(rnn_state, tuple):\n        # This would be an issue, LSTM state should be a tuple from encoder\n        # print(\"Warning: Expected tuple state for LSTM, got single tensor.\")\n        # return (rnn_state, torch.zeros_like(rnn_state)) # Attempt to construct a cell state\n    return rnn_state # Default pass-through\n\ndef inference(encoder, decoder, source_sequence, target_tensor, data, device_param, h_params, loss_fn, batch_num): # Names kept\n    encoder.eval()\n    decoder.eval()\n\n    accumulated_loss_for_batch = 0.0 # Internal name change\n    correctly_predicted_sequences = 0 # Internal name change\n\n    # Redundant op for batch_num\n    _ = batch_num + 1\n\n    with torch.no_grad():\n        # Encoder pass\n        encoder_initial_hidden = encoder.getInitialState()\n        if h_params[\"cell_type\"].upper() == \"LSTM\":\n            # LSTM state is a tuple (hidden, cell)\n            encoder_initial_hidden = (encoder_initial_hidden, encoder.getInitialState())\n\n        # The encoder output (sequence of hidden states) is often not used directly in basic seq2seq\n        # if attention is not present. Only the final state is used.\n        _, encoder_final_context_state = encoder(source_sequence, encoder_initial_hidden)\n\n        # Decoder setup\n        decoder_current_hidden_state = _unpack_rnn_state(encoder_final_context_state, h_params[\"cell_type\"])\n\n        current_batch_size = source_sequence.size(0) # Get dynamic batch size\n        decoder_input_token_tensor = _prepare_initial_decoder_input(\n            current_batch_size, # Use dynamic batch size\n            data['target_char_index'][START_TOKEN],\n            device_param\n        )\n\n        all_predicted_tokens_for_batch = []\n\n        # Decoder loop (step-by-step generation)\n        for dec_step_idx in range(data[\"OUTPUT_MAX_LENGTH\"]):\n            # Get actual target tokens for this step (for loss calculation)\n            actual_target_tokens_this_step = target_tensor[:, dec_step_idx]\n\n            # Decoder forward pass for one step\n            # Output is (Batch, Seq=1, VocabSize)\n            decoder_output_log_probs, decoder_current_hidden_state = decoder(\n                decoder_input_token_tensor,\n                decoder_current_hidden_state\n            )\n\n            # Squeeze out the Seq=1 dimension for loss calculation and topk\n            # Result: (Batch, VocabSize)\n            current_step_log_probs = decoder_output_log_probs.squeeze(1)\n\n            # Loss calculation for this step\n            loss_this_step = loss_fn(current_step_log_probs, actual_target_tokens_this_step)\n            accumulated_loss_for_batch += loss_this_step.item() # .item() to get Python number\n\n            # Get top prediction (greedy decoding)\n            # top_probs_values, top_token_indices = current_step_log_probs.topk(1, dim=1) # dim=1 for vocab\n            _, top_token_indices = current_step_log_probs.topk(1, dim=1)\n\n\n            # Prepare predicted token as next input\n            decoder_input_token_tensor = top_token_indices # Already (Batch, 1) after topk(1)\n            # No detach needed here due to torch.no_grad() context, but good practice if outside\n\n            all_predicted_tokens_for_batch.append(decoder_input_token_tensor.squeeze(1)) # Squeeze to (Batch) for stacking\n\n        # Assemble all predictions\n        # Stack along new dimension (dim=1) -> (Batch, OutputLength)\n        if all_predicted_tokens_for_batch:\n            batch_predictions_tensor = torch.stack(all_predicted_tokens_for_batch, dim=1)\n        else: # Should not happen\n            batch_predictions_tensor = torch.empty(current_batch_size, 0, dtype=torch.long, device=device_param)\n\n        # Calculate number of full sequences correctly predicted\n        # Element-wise comparison, then .all() across sequence dimension, then sum correct sequences\n        if batch_predictions_tensor.size(1) == target_tensor.size(1): # Ensure lengths match for comparison\n            correctly_predicted_sequences = (batch_predictions_tensor == target_tensor).all(dim=1).sum().item()\n        else:\n            # This case implies a bug in OUTPUT_MAX_LENGTH or loop, predictions don't match target length\n            print(\"Warning: Prediction tensor length mismatch with target tensor in inference.\")\n            correctly_predicted_sequences = 0\n\n\n    average_loss_over_steps = accumulated_loss_for_batch / data[\"OUTPUT_MAX_LENGTH\"] if data[\"OUTPUT_MAX_LENGTH\"] > 0 else 0\n    return correctly_predicted_sequences, average_loss_over_steps\n\n\ndef evaluate(encoder, decoder, data, dataloader, device_param, h_params, loss_fn): # Names kept\n    total_correct_predictions_eval = 0\n    total_loss_eval = 0.0\n    num_samples_in_dataloader = len(dataloader.dataset)\n    num_batches_in_dataloader = len(dataloader)\n\n    # Redundant counter\n    eval_batch_counter = 0\n\n    for batch_idx_eval, (source_batch_eval, target_batch_eval) in enumerate(dataloader):\n        # Assuming data is already on device from dataloader if MyDataset handles it\n        # input_tensor_eval = source_batch_eval.to(device_param)\n        # target_tensor_eval = target_batch_eval.to(device_param)\n        input_tensor_eval = source_batch_eval\n        target_tensor_eval = target_batch_eval\n\n        # Get dynamic batch size for this batch (could be smaller for last batch)\n        # current_eval_batch_size = input_tensor_eval.size(0)\n        # Note: h_params[\"batch_size\"] is used in inference, which might be an issue for last batch if not full.\n        # The refactored inference now uses source_sequence.size(0)\n\n        correct_count_batch, loss_value_batch = inference(\n            encoder, decoder,\n            input_tensor_eval, target_tensor_eval,\n            data, device_param, h_params, loss_fn, batch_idx_eval\n        )\n\n        total_correct_predictions_eval += correct_count_batch\n        total_loss_eval += loss_value_batch\n        eval_batch_counter += 1\n\n    # Final metrics calculation\n    overall_accuracy = total_correct_predictions_eval / num_samples_in_dataloader if num_samples_in_dataloader > 0 else 0.0\n    average_loss_across_batches = total_loss_eval / num_batches_in_dataloader if num_batches_in_dataloader > 0 else 0.0\n\n    # Sanity check for counter\n    # if eval_batch_counter != num_batches_in_dataloader:\n        # print(f\"Warning: Mismatch in evaluated batch count ({eval_batch_counter} vs {num_batches_in_dataloader}).\")\n\n    return overall_accuracy, average_loss_across_batches\n\n# --- Utility for String Conversion (Using VocabularyManager style) ---\ndef make_strings(data_map_dict, source_indices_tensor, target_indices_tensor, output_indices_tensor): # Names kept\n    \"\"\"Converts index tensors to strings using char_index_char maps from data_map_dict.\"\"\"\n\n    # Reconstruct source string\n    source_char_list = []\n    for idx_val in source_indices_tensor:\n        # item() gets Python number from 0-dim tensor\n        source_char_list.append(data_map_dict['source_index_char'].get(idx_val.item(), PAD_TOKEN))\n    reconstructed_source_str = \"\".join(source_char_list)\n\n    # Reconstruct target string\n    target_char_list = []\n    # Using a while loop for variation\n    k = 0\n    while k < len(target_indices_tensor):\n        idx_val = target_indices_tensor[k]\n        target_char_list.append(data_map_dict['target_index_char'].get(idx_val.item(), PAD_TOKEN))\n        k += 1\n    reconstructed_target_str = \"\".join(target_char_list)\n\n    # Reconstruct output string\n    output_char_list = [data_map_dict['target_index_char'].get(idx_val.item(), PAD_TOKEN) for idx_val in output_indices_tensor]\n    reconstructed_output_str = \"\".join(output_char_list)\n\n    # Redundant operation for code structure change\n    final_tuple = (reconstructed_source_str, reconstructed_target_str, reconstructed_output_str)\n    return final_tuple[0], final_tuple[1], final_tuple[2]\n\n\n# --- Training Loop (Refactored More Significantly) ---\n\ndef _initialize_optimizers(enc_model, dec_model, opt_name_str, learn_rate):\n    \"\"\"Helper to initialize optimizers for encoder and decoder.\"\"\"\n    opt_name_lower = opt_name_str.lower()\n    OptimizerClass = None\n    if opt_name_lower == \"adam\":\n        OptimizerClass = optim.Adam\n    elif opt_name_lower == \"nadam\":\n        OptimizerClass = optim.NAdam\n    # Add SGD or others if needed\n    # elif opt_name_lower == \"sgd\":\n    #     OptimizerClass = optim.SGD\n    else:\n        print(f\"Unsupported optimizer '{opt_name_str}', defaulting to Adam.\")\n        OptimizerClass = optim.Adam # Fallback\n\n    enc_optimizer = OptimizerClass(enc_model.parameters(), lr=learn_rate)\n    dec_optimizer = OptimizerClass(dec_model.parameters(), lr=learn_rate)\n    return enc_optimizer, dec_optimizer\n\n\ndef _training_step_for_batch(encoder_model, decoder_model,\n                             source_batch, target_batch,\n                             encoder_opt, decoder_opt,\n                             h_params_cfg, data_struct, loss_criterion,\n                             epoch_num_info, batch_num_info): # More descriptive param names\n    \"\"\"Performs a single training step (forward, loss, backward, step) for a batch.\"\"\"\n\n    # --- Encoder Pass ---\n    encoder_initial_hidden = encoder_model.getInitialState()\n    if h_params_cfg[\"cell_type\"].upper() == \"LSTM\":\n        encoder_initial_hidden = (encoder_initial_hidden, encoder_model.getInitialState())\n\n    # Encoder output sequence is not always directly used by a simple decoder, only final state\n    _, encoder_final_context = encoder_model(source_batch, encoder_initial_hidden)\n\n    # --- Decoder Pass & Loss Calculation ---\n    batch_loss_total = 0.0\n\n    decoder_current_state = _unpack_rnn_state(encoder_final_context, h_params_cfg[\"cell_type\"]) # Pass LSTM tuple if needed\n\n    # Dynamic batch size for this iteration\n    current_iter_batch_size = source_batch.size(0)\n\n    # Initial decoder input: <START> token (using target_batch[:,0] for first step was original logic)\n    # Let's stick to target_batch[:,0] as the very first input.\n    # This means the decoder is \"shown\" the first actual target token.\n    # This is a form of teacher forcing for the *first token only*, regardless of TEACHER_FORCING_RATIO.\n    decoder_current_input_token_seq = target_batch[:, 0].view(current_iter_batch_size, 1)\n\n    # For sequence accuracy calculation\n    batch_predicted_token_indices_list = []\n\n    # Teacher forcing decision for the rest of the sequence\n    use_tf_this_batch = random.random() < TEACHER_FORCING_RATIO\n\n    # Redundant log for structure\n    # _ = print(f\"Epoch {epoch_num_info}, Batch {batch_num_info}: Teacher Forcing = {use_tf_this_batch}\") if batch_num_info % 100 == 0 else None\n\n    max_output_len_for_loop = data_struct[\"OUTPUT_MAX_LENGTH\"]\n\n    for dec_step_idx_train in range(max_output_len_for_loop):\n        actual_target_tokens_at_this_step = target_batch[:, dec_step_idx_train]\n\n        # Decoder forward for one step\n        # Input: (Batch, Seq=1), Output: (Batch, Seq=1, VocabSize)\n        decoder_output_log_probs_step, decoder_current_state = decoder_model(\n            decoder_current_input_token_seq,\n            decoder_current_state\n        )\n\n        # Squeeze for loss: (Batch, VocabSize)\n        current_step_output_for_loss = decoder_output_log_probs_step.squeeze(1)\n\n        # Calculate loss for this step\n        loss_for_current_step = loss_criterion(current_step_output_for_loss, actual_target_tokens_at_this_step)\n        batch_loss_total += loss_for_current_step # Accumulate raw loss, will average later\n\n        # Determine next input for the decoder\n        # Get greedy prediction: (Batch, 1)\n        _, top_predicted_indices_this_step = current_step_output_for_loss.topk(1, dim=1)\n\n        # Store prediction for accuracy calculation (squeeze to Batch for list)\n        batch_predicted_token_indices_list.append(top_predicted_indices_this_step.squeeze(1).detach())\n\n        if dec_step_idx_train < max_output_len_for_loop - 1: # If not the last step\n            if use_tf_this_batch:\n                # Teacher forcing: use actual next target token\n                decoder_current_input_token_seq = target_batch[:, dec_step_idx_train + 1].view(current_iter_batch_size, 1)\n            else:\n                # No teacher forcing: use decoder's own prediction\n                decoder_current_input_token_seq = top_predicted_indices_this_step # Already (Batch,1)\n        # else: last step, no need to prepare next input\n\n    # --- Accuracy and Loss Finalization for Batch ---\n    # Stack predictions: (Batch, OutputLength)\n    if batch_predicted_token_indices_list:\n        batch_predictions_final_tensor = torch.stack(batch_predicted_token_indices_list, dim=1)\n    else:\n        batch_predictions_final_tensor = torch.empty(current_iter_batch_size, 0, dtype=torch.long, device=target_batch.device)\n\n\n    num_correct_sequences_in_batch = 0\n    if batch_predictions_final_tensor.size(1) == target_batch.size(1):\n        num_correct_sequences_in_batch = (batch_predictions_final_tensor == target_batch).all(dim=1).sum().item()\n    else:\n        print(\"Warning: Prediction length mismatch in training step.\")\n\n\n    # Average loss over sequence length for this batch\n    average_loss_for_this_batch = batch_loss_total / max_output_len_for_loop if max_output_len_for_loop > 0 else 0.0\n\n    # --- Backpropagation ---\n    encoder_opt.zero_grad()\n    decoder_opt.zero_grad()\n\n    # Backward pass on the averaged loss for the batch\n    # average_loss_for_this_batch.backward() # If average_loss_for_this_batch is still a tensor\n    # If batch_loss_total was summed from tensor losses, it should still have graph\n    if isinstance(batch_loss_total, torch.Tensor) and batch_loss_total.requires_grad:\n         scaled_loss = batch_loss_total / max_output_len_for_loop # Keep as tensor for backward\n         scaled_loss.backward()\n    elif isinstance(average_loss_for_this_batch, torch.Tensor) and average_loss_for_this_batch.requires_grad : # Should be the case\n        average_loss_for_this_batch.backward()\n    else: # If it became a float due to .item() too early\n        print(\"Error: Loss is not a tensor, cannot backpropagate.\")\n\n\n    # Optional: Gradient Clipping\n    # torch.nn.utils.clip_grad_norm_(encoder_model.parameters(), max_norm=1.0)\n    # torch.nn.utils.clip_grad_norm_(decoder_model.parameters(), max_norm=1.0)\n\n    encoder_opt.step()\n    decoder_opt.step()\n\n    # Return loss as float for accumulation\n    final_batch_loss_float = average_loss_for_this_batch.item() if isinstance(average_loss_for_this_batch, torch.Tensor) else average_loss_for_this_batch\n\n    return num_correct_sequences_in_batch, final_batch_loss_float\n\n\ndef train_loop(encoder, decoder, h_params, data, train_dataloader, val_dataloader, device_param): # Names kept\n    encoder_optimizer, decoder_optimizer = _initialize_optimizers(\n        encoder, decoder, h_params[\"optimizer\"], h_params[\"learning_rate\"]\n    )\n\n    num_train_samples_total = len(train_dataloader.dataset)\n    num_train_batches_total = len(train_dataloader)\n\n    loss_criterion_instance = nn.NLLLoss() # NLLLoss because LogSoftmax is in Decoder\n\n    # Training timer\n    overall_training_start_time = time.monotonic()\n\n    for epoch_count in range(h_params[\"epochs\"]): # epoch_count is 0-indexed\n        epoch_timer_start = time.monotonic()\n\n        encoder.train() # Set to train mode\n        decoder.train() # Set to train mode\n\n        current_epoch_total_loss = 0.0\n        current_epoch_total_correct = 0\n\n        # Iterate over training batches\n        for batch_idx_train, (source_data_batch, target_data_batch) in enumerate(train_dataloader):\n            # Data to device (should be handled by MyDataset + DataLoader if device is passed early)\n            s_batch = source_data_batch #.to(device_param)\n            t_batch = target_data_batch #.to(device_param)\n\n            batch_correct_count, batch_avg_loss = _training_step_for_batch(\n                encoder, decoder, s_batch, t_batch,\n                encoder_optimizer, decoder_optimizer,\n                h_params, data, loss_criterion_instance,\n                epoch_count, batch_idx_train\n            )\n\n            current_epoch_total_correct += batch_correct_count\n            current_epoch_total_loss += batch_avg_loss # Already averaged per sequence\n\n            # Optional: Log batch progress\n            # if (batch_idx_train + 1) % 200 == 0: # Print every 200 batches\n                # print(f\"  Epoch {epoch_count+1}, Batch {batch_idx_train+1}/{num_train_batches_total}, Batch Loss: {batch_avg_loss:.4f}\")\n\n        # Calculate epoch metrics for training\n        avg_train_loss_epoch = current_epoch_total_loss / num_train_batches_total if num_train_batches_total > 0 else 0.0\n        train_accuracy_epoch = current_epoch_total_correct / num_train_samples_total if num_train_samples_total > 0 else 0.0\n\n        # Validation step\n        val_accuracy, val_loss = evaluate(encoder, decoder, data, val_dataloader, device_param, h_params, loss_criterion_instance)\n\n        epoch_duration_secs = time.monotonic() - epoch_timer_start\n\n        # Logging (print and WandB)\n        print_log_msg = (\n            f\"Epoch: {epoch_count+1}/{h_params['epochs']} | Time: {epoch_duration_secs:.2f}s | \"\n            f\"Train Acc: {train_accuracy_epoch*100:.2f}% | Train Loss: {avg_train_loss_epoch:.4f} | \"\n            f\"Val Acc: {val_accuracy*100:.2f}% | Val Loss: {val_loss:.4f}\"\n        )\n        print(print_log_msg)\n\n        try:\n            wandb.log({\n                \"train_accuracy\": train_accuracy_epoch,\n                \"train_loss\": avg_train_loss_epoch,\n                \"val_accuracy\": val_accuracy,\n                \"val_loss\": val_loss,\n                \"epoch_duration_sec\": epoch_duration_secs,\n                \"epoch\": epoch_count + 1 # Log 1-based epoch for clarity\n            })\n        except Exception as e:\n            print(f\"WandB logging failed for epoch {epoch_count+1}: {e}\")\n\n        # Clean up GPU memory if applicable\n        if device_param.type == 'cuda':\n            # gc.collect() # Python GC\n            torch.cuda.empty_cache() # PyTorch CUDA cache\n\n    total_train_time_secs = time.monotonic() - overall_training_start_time\n    print(f\"Total training duration: {total_train_time_secs // 60:.0f}m {total_train_time_secs % 60:.0f}s\")\n\n    return encoder, decoder, loss_criterion_instance # Return trained models and loss_fn\n\n# --- Main Training Function ---\ndef train(h_params, data, device_param, train_dataloader, val_dataloader): # Names kept\n    # Model Initialization\n    # These models are moved to 'device_param' within their constructors if device_param is passed,\n    # or can be moved here explicitly. The current Encoder/Decoder takes device_param.\n    encoder_model_instance = Encoder(h_params, data, device_param).to(device_param) # Explicit .to(device) for safety\n    decoder_model_instance = Decoder(h_params, data, device_param).to(device_param)\n\n    # Redundant logging of model parameters (example)\n    # total_params_enc = sum(p.numel() for p in encoder_model_instance.parameters() if p.requires_grad)\n    # total_params_dec = sum(p.numel() for p in decoder_model_instance.parameters() if p.requires_grad)\n    # print(f\"Encoder trainable parameters: {total_params_enc}\")\n    # print(f\"Decoder trainable parameters: {total_params_dec}\")\n\n    # Execute training loop\n    final_encoder, final_decoder, final_loss_fn = train_loop(\n        encoder_model_instance, decoder_model_instance,\n        h_params, data,\n        train_dataloader, val_dataloader,\n        device_param\n    )\n    return final_encoder, final_decoder, final_loss_fn\n\n\n# --- Dataloader Preparation (Refactored) ---\ndef prepare_dataloaders(train_source_list, train_target_list, val_source_list, val_target_list, h_params_cfg): # Names kept\n\n    # --- Training Data Preparation ---\n    # Preprocess training data (this creates vocabs based on training data only)\n    # Using copy.deepcopy for safety if original lists are modified elsewhere, though arrays are usually copied by pd.to_numpy()\n    data_processing_object = preprocess_data(copy.deepcopy(train_source_list), copy.deepcopy(train_target_list))\n\n    # Extract sequences for training dataset\n    training_data_source_seq = data_processing_object[\"source_data_seq\"]\n    training_data_target_seq = data_processing_object['target_data_seq']\n\n    train_dataset_instance = MyDataset((training_data_source_seq, training_data_target_seq))\n\n    # Training DataLoader\n    train_dl = DataLoader(\n        train_dataset_instance,\n        batch_size=h_params_cfg[\"batch_size\"],\n        shuffle=True, # Shuffle training data\n        drop_last=True, # Important for consistent batch sizes if not perfectly divisible\n        num_workers=0, # Set to > 0 for parallel data loading if beneficial and safe\n        pin_memory=True if device.type == 'cuda' else False # For faster CPU to GPU transfers\n    )\n\n    # --- Validation Data Preparation ---\n    # Use the vocabularies learned from the training set\n    source_vocab_mgr_val = data_processing_object[\"_source_vocab_manager\"]\n    target_vocab_mgr_val = data_processing_object[\"_target_vocab_manager\"]\n\n    # Pad validation strings using max_lengths from training data\n    val_padded_s_strings = add_padding(val_source_list, data_processing_object[\"INPUT_MAX_LENGTH\"])\n    val_padded_t_strings = add_padding(val_target_list, data_processing_object[\"OUTPUT_MAX_LENGTH\"])\n\n    # Convert validation strings to sequences using training vocabs\n    val_source_sequences = generate_string_to_sequence(val_padded_s_strings, source_vocab_mgr_val, device)\n    val_target_sequences = generate_string_to_sequence(val_padded_t_strings, target_vocab_mgr_val, device)\n\n    validation_dataset_instance = MyDataset((val_source_sequences, val_target_sequences))\n\n    # Validation DataLoader\n    val_dl = DataLoader(\n        validation_dataset_instance,\n        batch_size=h_params_cfg[\"batch_size\"],\n        shuffle=False, # No need to shuffle validation data\n        drop_last=True, # Or False, depending on if you need to evaluate on every last sample\n        num_workers=0,\n        pin_memory=True if device.type == 'cuda' else False\n    )\n\n    # Redundant operation for code structure\n    # _ = len(train_dl) * len(val_dl)\n\n    return train_dl, val_dl, data_processing_object # Return the main data object for char maps etc.\n\n\n# --- Hyperparameters and WandB Sweep Configuration ---\n# Example h_params (original, commented out as it's for single run/sweep)\n# h_params = {\n# \"char_embd_dim\": 256,\n# \"hidden_layer_neurons\": 256,\n# \"batch_size\": 32,\n# \"number_of_layers\": 3,\n# \"learning_rate\": 0.0001,\n# \"epochs\": 20,\n# \"cell_type\": \"LSTM\",\n# \"dropout\": 0.1,\n# \"optimizer\": \"adam\"\n# }\n\n# Sweep parameters (original structure)\nsweep_params = {\n    'method' : 'bayes',\n    'name'   : 'DL_A3_Highly_Refactored_Sweep_v2', # New sweep name\n    'metric' : {\n        'goal' : 'maximize',\n        'name' : 'val_accuracy',\n    },\n    'parameters' : { # Slightly adjusted ranges for variety\n        'epochs':{'values' : [10, 15, 20]},\n        'learning_rate':{'values' : [0.001, 0.0005, 0.0002]}, # Finer control\n        'batch_size':{'values':[32, 64]}, # Keeping batch sizes manageable\n        'char_embd_dim':{'values' : [128, 256, 384] } ,\n        'number_of_layers':{'values' : [1, 2, 3]},\n        'optimizer':{'values':['nadam','adam']},\n        'cell_type':{'values' : [\"LSTM\", \"GRU\"]},\n        'hidden_layer_neurons':{'values': [256, 384, 512]},\n        'dropout':{'values': [0.1, 0.2, 0.3, 0.0]} # Added 0.0 for no dropout\n    }\n}\n\n# To initialize a sweep (run this part once, e.g., in a separate script or notebook cell):\nSWEEP_ID_GENERATED = wandb.sweep(sweep=sweep_params, project=\"DA6401_Assignment_3\")\nprint(f\"Generated WandB Sweep ID: {SWEEP_ID_GENERATED}\")\n# Store this SWEEP_ID_GENERATED. For this example, I'll use the one from the prompt.\n\n# Main function for WandB agent\ndef main(): # Function name kept (for wandb.agent)\n    run_config = None # To hold wandb.config\n    try:\n        # Initialize a new WandB run for this agent's trial\n        # project name might be needed if not running via CLI `wandb agent` command that specifies it\n        run_instance = wandb.init(project=\"DL Assignment 3\") # Name and config will be set by sweep\n        run_config = wandb.config\n\n        # Construct a descriptive run name for better tracking in WandB UI\n        # This uses attribute access which is standard for wandb.config\n        current_run_name = (\n            f\"{run_config.cell_type}_{run_config.optimizer}_ep{run_config.epochs}_\"\n            f\"lr{run_config.learning_rate:.0e}_emb{run_config.char_embd_dim}_\"\n            f\"hid{run_config.hidden_layer_neurons}_bs{run_config.batch_size}_\"\n            f\"layers{run_config.number_of_layers}_drop{run_config.dropout:.1f}\"\n        ).replace(\"e-0\", \"e-\") # Cleaner scientific notation for LR\n\n        wandb.run.name = current_run_name # Update the run name in WandB\n        # wandb.run.save() # Not always necessary, name update usually syncs\n\n        # print(f\"--- Starting WandB Run: {current_run_name} ---\")\n        # print(\"Hyperparameters for this run:\")\n        # for key, val in run_config.items():\n        #     print(f\"  {key}: {val}\")\n\n    except Exception as e:\n        print(f\"Error during WandB initialization or run naming: {e}\")\n        # If wandb fails, run_config might be None. We might want to exit or use default h_params.\n        # For now, assume it will proceed and crash later if run_config is vital and None.\n        # Or, define a default_h_params here for fallback.\n        if run_config is None: # Critical failure\n            print(\"Cannot proceed without wandb config in sweep mode. Exiting this agent run.\")\n            return # Exit this agent's attempt\n\n    # Ensure all h_params needed by the functions are present in run_config\n    # This acts as a contract check or allows defaults.\n    # Example: run_config.setdefault('some_new_param', default_value)\n\n    # Prepare dataloaders using the hyperparameters from wandb.config\n    # train_source, train_target, etc., are global from data loading section\n    train_dataloader_sweep, val_dataloader_sweep, data_struct_sweep = prepare_dataloaders(\n        train_source, train_target,\n        val_source, val_target,\n        run_config # Pass the wandb config object\n    )\n\n    # Execute the training process\n    # The 'train' function internally logs metrics to the current WandB run\n    train(run_config, data_struct_sweep, device, train_dataloader_sweep, val_dataloader_sweep)\n\n    # print(f\"--- Finished WandB Run: {current_run_name} ---\")\n    # WandB run finishes automatically when the agent function exits or wandb.finish() is called.\n    # wandb.finish() # Explicitly finish if desired, though agent usually handles it.\n\n# --- Agent Execution ---\n# Replace \"YOUR_PROJECT_NAME/YOUR_SWEEP_ID\" with the actual ID from wandb.sweep()\n# The prompt used \"hw3b5jng\" which looks like just the sweep ID part.\n# Usually it's <entity>/<project>/<sweep_id>\n# AGENT_SWEEP_ID = \"hw3b5jng\" \nPROJECT_NAME_FOR_AGENT = \"DA6401_Assignment_3\" # Explicitly state project for agent\n\nif __name__ == \"__main__\":\n    # This block allows running a single training session with fixed params if not doing a sweep\n    # by commenting out the wandb.agent line.\n\n    # --- For Single Run (manual hyperparameter setting) ---\n    # print(\"Attempting a single, non-sweep run configuration...\")\n    # fixed_h_params = { # Define your fixed hyperparameters here\n    #     \"char_embd_dim\": 128, \"hidden_layer_neurons\": 256, \"batch_size\": 32,\n    #     \"number_of_layers\": 2, \"learning_rate\": 0.001, \"epochs\": 5, # Short epochs for test\n    #     \"cell_type\": \"GRU\", \"dropout\": 0.1, \"optimizer\": \"adam\"\n    # }\n    # single_run_wandb_name = (\n    #         f\"SINGLE_{fixed_h_params['cell_type']}_{fixed_h_params['optimizer']}_ep{fixed_h_params['epochs']}_\"\n    #         f\"lr{fixed_h_params['learning_rate']:.0e}_emb{fixed_h_params['char_embd_dim']}\"\n    # ).replace(\"e-0\", \"e-\")\n    try:\n        wandb.init(project=PROJECT_NAME_FOR_AGENT, name=single_run_wandb_name, config=fixed_h_params)\n        train_dl_single, val_dl_single, data_s_single = prepare_dataloaders(\n            train_source, train_target, val_source, val_target, fixed_h_params\n        )\n        train(fixed_h_params, data_s_single, device, train_dl_single, val_dl_single)\n        wandb.finish()\n        print(\"Single run finished.\")\n    except Exception as e_single:\n        print(f\"Error during single run: {e_single}\")\n    --- End Single Run Block ---\n\n    # --- For WandB Sweep ---\n    # To run the sweep, comment out the single run block above and uncomment the agent line.\n    print(f\"Attempting to start WandB agent for sweep ID: {AGENT_SWEEP_ID} in project: {PROJECT_NAME_FOR_AGENT}\")\n    try:\n        wandb.agent(sweep_id=AGENT_SWEEP_ID, function=main, count=100, project=PROJECT_NAME_FOR_AGENT)\n        print(\"WandB agent process finished or was stopped.\")\n    except Exception as e_agent:\n        print(f\"Error starting or running WandB agent: {e_agent}\")\n        print(\"Ensure you have run 'wandb login' and the sweep ID is correct for the specified project.\")\n    # --- End Sweep Block ---","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T11:53:47.406400Z","iopub.execute_input":"2025-05-12T11:53:47.406665Z","iopub.status.idle":"2025-05-12T11:53:55.469888Z","shell.execute_reply.started":"2025-05-12T11:53:47.406645Z","shell.execute_reply":"2025-05-12T11:53:55.469017Z"}},"outputs":[],"execution_count":null}]}